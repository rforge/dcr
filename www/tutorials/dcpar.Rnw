\documentclass[article,shortnames,nojss]{jss}
\usepackage{thumbpdf}

%% need no \usepackage{Sweave.sty}
\SweaveOpts{engine = R, strip.white = TRUE, keep.source = TRUE, eps = FALSE}

\let\code=\texttt
\let\proglang=\textsf
%% \newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}}
\newcommand{\R}{\proglang{R}}
\newcommand{\class}[1]{`\code{#1}'}
\renewenvironment{Schunk}{\small}{}

%\VignetteIndexEntry{dcpar paper}
%\VignettePackage{dcpar}
%\VignetteDepends{dclone}
%\VignetteKeywords{Bayesian statistics, data cloning, maximum likelihood inference, generalized linear models, R, parallel computing}

\author{P\'eter S\'olymos\\University of Alberta}
\Plainauthor{P\'eter S\'olymos}

\title{\pkg{dcpar}: Parallel Computing with MCMC and Data Cloning}
\Plaintitle{dcpar: parallel computing with MCMC and data cloning}
\Shorttitle{Parallel computing with MCMC and data cloning}

\Abstract{
Data cloning is a resource intensive global optimization technique, that
uses Bayesian MCMC tools to get maximum likelihood estimates and corresponding
standard errors. The \pkg{dcpar} \proglang{R} package is an extension for the
\pkg{dclone} package for enabling parallel computing for
MCMC chains and fitting the same models with different number of clones
of the data set. Processing time is almost linearly related
to the size of the problems considered, consequently considerable
computing time decrease can be gained by exploiting multiple core
machines or cluster of workstations.
}

\Keywords{Bayesian statistics, data cloning, maximum likelihood inference, generalized linear models, \proglang{R}, parallel computing}
\Plainkeywords{Bayesian statistics, data cloning, maximum likelihood inference, generalized linear models, R, parallel computing}

\Address{
  P\'eter S\'olymos\\
  Alberta Biodiversity Monitoring Institute\\
  and Boreal Avian Modelling Project
  Department of Biological Sciences\\
  CW 405, Biological Sciences Bldg\\
  University of Alberta\\
  Edmonton, Alberta, T6G 2E9, Canada\\
  E-mail: \email{solymos@ualberta.ca}\\
}

\begin{document}


<<echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+   ", useFancyQuotes = FALSE, width = 76)
@


\section{Introduction}

Data cloning is a statistical computing method introduced by \citet{Lele2007}.
It is a global optimization technique, that exploits the computational simplicity of the Markov chain Monte Carlo (MCMC) algorithms used in the Bayesian statistical framework, but it provides valid frequentist inferences such as the maximum likelihood estimates and their standard errors.
The basic idea of data cloning is, that if we copy identical clones of the data $k$ times, we can approach the asymptotic maximum likelihood estimates corresponding to $k$ times the log-likelihood.
This approach enables to fit complex hierarchical models (\citep{Lele2007}, \citep{Ponciano2009}) and helps in studying parameter identifiability (cit here), but comes with a price in ptocessing time that increases with the number of clones. 

The \pkg{dclone} \proglang{R} (cite R) package \citep{Solymos2009} aims to provide low level functionality to easily implement 
more specific higher level procedures based on data cloning for users familiar with the Bayesian methodology.
The \pkg{dcpar} package (sit dcpar) extends the \pkg{dclone} for parallel computations building on the infrastructure provided by the \pkg{snow} package (cit here). This paper I demonstrate how computing time can improve by using \pkg{dcpar}, and how can the parallelization effectively been optimized. The \pkg{dcpar} package currently
supports the \proglang{JAGS} software (cit), but future development might include to support other MCMC software (e.g. WinBUGS and OpenBUGS, cit, cit).

\section{Ways of parallelization}

There is no universal number of clones that is needed to reach convergence (the asymptotic joint distribution of the parameters is degenerate multivariate normal), so $k$ has to be determined empirically, by fitting the same model with the data of different number of clones, and checking if the variances of the parameters are going to zero. For example, if we fit the same model with $k=(1,2,5,10,20)$, it means that we have to fit five models. Without parallelization, we can use subsequent calls of the \code{jags.fit} function of the \pkg{dclone} \proglang{R} package with different cloned data sets (corresponding to elements in $k$), or use the \code{dc.fit} wrapper function.

Parallelization can happen in two ways: (1) we parallelize the computation of the MCMC chains (ususally we use more than one chain to chack proper mixing behavour of the model); or (2) we split the problem into subsets and fit the models in each subset on a different worker of the parallelized computing environment.
The first type of parallelization can be addressed by the \code{jags.parfit} function (parallelized version of \code{jags.fit}), while the second type can be addressed by the \code{dc.parfit} function (parallelized version of \code{dc.fit}). Both \code{jags.parfit} and \code{dc.parfit} is built upon the parallel computing infrastructure provided by the \pkg{snow} package, with some additional convenince functions (see next sections). The most important additional function is the \code{snowWrapper}, which unites several basic \pkg{snow} function (i.e.~\code{clusterCall}, \code{clusterEvalQ}, \code{clusterApply}), and most importantly can execute the \code{clusterExport} function call from within a function (where objects are not defined in the global environment).

\section{Example data and the Bayesian model}

We will use a simulated data set correponding to the Poisson generlaized linear model with random intercept used by \citet{Solymos2009} for i.i.d.~observations of $Y_{i}$ counts from $i = 1,2,\ldots,n$ localities: 
\begin{eqnarray*}
(Y_{i} \mid \lambda_{i}) & \sim & \mathrm{Poisson}(\lambda_{i})\\
\log(\lambda_{i}) & = & \beta_{0i} + X_{i} \beta_{1}\\
\beta_{0i} & \sim & \mathrm{Normal}(\beta_{0}, \sigma^2).
\end{eqnarray*}
The \proglang{R} code for the data generation corresponding to this model is 
($n=200$, $\beta_{0}=1$, $\beta_{1}=-1$, $\sigma=0.25$):

<<echo=false,results=hide>>=
library(dcpar)
setwd("c:/svn/abmiserver/dclone-paper")
load("results.Rdata")
@

<<>>=
n <- 200
beta <- c(1, -1)
sigma <- 0.25
set.seed(1234)
x <- runif(n)
X <- model.matrix(~x)
mu <- rnorm(n, mean=drop(X %*% beta), sd=sigma)
Y <- rpois(n, exp(mu))
@
We put these object into a list (this will be the data argument):
<<>>=
dat <- list(Y = Y, X = X, n = n, np = NCOL(X))
@
The corresponding \proglang{JAGS} model is:
<<>>=
glmm.model <- function() {
    for (i in 1:n) {
        Y[i] ~ dpois(exp(mu[i]))
        mu[i] ~ dnorm(inprod(X[i,], beta), 1/exp(log.sigma)^2)
    }
    for (j in 1:np) {
        beta[j] ~ dnorm(0, 0.001)
    }
    log.sigma ~ dnorm(0, 0.001)
}
@
Note that we used $log(\sigma)$ with Normal prior distribution to enhance chanin mixing, and because of the asymptotic multivariate normality involved in the theory of data cloning \citap{Lele2007}. We are using the following settings for fitting the \proglang{JAGS} model:
<<>>=
n.adapt <- 2000
n.update <- 8000
n.iter <- 2000
@
And finally set the vector of \code{k} for the number of clones to be used:
<<>>=
k <- c(1, 2, 5, 10, 20)
@

\section{Parallel MCMC chains}

%%% ADD HERE MORE CONTEXT ON PARALLEL BAYESIAN ANALYSES !!!!!!!!!! %%%%%%%%%%

This simple function allows us to switch parallelization on/off (by setting the \code{parallel} argument as \code{TRUE}/\code{FALSE}, respectively), and also measure the time elapsed in minutes:
<<>>=
timerfitfun <- function(parallel = FALSE, ...) {
    t0 <- proc.time()
    mod <- if (parallel)
        jags.parfit(...) else jags.fit(...)
    attr(mod, "timer") <- (proc.time() - t0)[3] / 60
    mod
}
@

First we fit the five models sequentially (one model for each element of \code{k}) without parallelization of the three MCMC chains:
<<eval=false>>=
res1 <- lapply(k, function(z)
    timerfitfun(parallel = FALSE,
        dclone(dat, z, multiply = "n", unchanged = "np"),
        c("beta", "log.sigma"), glmm.model,
        n.adapt = n.adapt, n.update = n.update, n.iter = n.iter))
@

Now we fit the models by using MCMC chains computed on parallel workers. We use three workers (one worker for each MCMC chain, the type we use here is ``socket'', but type can be anything else accepted by the \pkg{snow} package), then fit the five models sequentially, and close the connection in the end of the process:
<<eval=false>>=
cl <- makeCluster(3, type = "SOCK")
res2 <- lapply(k, function(z)
    timerfitfun(parallel = TRUE, cl,
        dclone(dat, z, multiply = "n", unchanged = "np"),
        c("beta", "log.sigma"), glmm.model,
        n.adapt = n.adapt, n.update = n.update, n.iter = n.iter))
stopCluster(cl)
@

We can extract timer information from the result objects:
<<>>=
pt1 <- sapply(res1, function(z) attr(z, "timer"))
pt2 <- sapply(res2, function(z) attr(z, "timer"))
@
Processing time (column \code{time}) increased almost linearly with $k$ (with and without parallelization, too), columns {rel.change} indicate change relative to $k=1$:
<<>>=
tab1 <- data.frame(n.clones=k,
    time=pt1, rel.change=pt1/pt1[1],
    time.par=pt2, rel.change.par=pt2/pt2[1])
round(tab1, 2)
@
It took \Sexpr{round(sum(pt1),2)} minutes to fit the five models without parallelization, and only \Sexpr{round(sum(pt2),2)} minutes with parallelization (\Sexpr{round(100*sum(pt2)/sum(pt1),2)} \%).
Parallel MCMC computations effectively reduced the processing time to 1 / (number of chains) = 1/3.

\section{Partitioning the problem into parallel subsets}

The other way of parallelizing the iterative fitting procedure with data cloning is to split the problem into subsets with respect to the problem size (i.e.~approximate processing time). As we saw in the previous section, processing time increased linearly with $k$. Consequently, processing time can be effectively approximated by $k$.

The \pkg{dcpar} has some underlying functions (e.g.~\code{clusterSplitSB}, \code{parLapplySB}\footnote{These function resemble in name to functions \code{clusterSplit} and \code{parLapply} in the \pkg{snow} package.}) developped to deal with \emph{size balancing}. In size balancing, the problems are re-ordered from largest to smallest, and then subsets are determined by minimizing the total approximate processing time.
This splitting is deterministic, thus computations are reproducible. However, size balancing can be combined with load balancing (cit here). This option is implemented in the \code{parLapplySLB} function. If size (processing time) is correct, this should be identical to size balancing, but the actual sequence of execution might depend on unequal performance of the workers. The splitting in this case is non-deterministic (might not be reproducible). Load balancing can be set as a global \pkg{dcpar} option by setting \code{options("dcpar.LB" = TRUE)} (this value is \code{FALSE} by default).

The \code{clusterSize} function can be used to determine the optimal number of workers needed for a given size vecor:
<<>>=
clusterSize(pt1)
@
Essentially, by using size balancing, only two workers are needed to accomplish the task.
The \code{clusterSize} can help to visualize the effect of using different balancing options (Fig.~\ref{fig:balance}):
\begin{figure}[tb]
\begin{center}
<<fig1,echo=true,fig=true,width=9,height=3>>=
clusterSize(pt1)
opar <- par(mfrow=c(1,3))
plotClusterSize(2, pt1, "none", col=heat.colors(length(k)))
plotClusterSize(2, pt1, "load", col=heat.colors(length(k)))
plotClusterSize(2, pt1, "size", col=heat.colors(length(k)))
par(opar)
@
\end{center}
\caption{Effect of different balancing types on approximate processing time by showing the subsets. Note, the actual sequance of execution might be different for load balancing, because it is non-deterministic.}
  \label{fig:balance}
\end{figure}
We can see in Fig.~\ref{fig:balance} that for two workers size balancing results in the shortest processing time. While the model with $k=\mathrm{max}(k)$ is running, all the other models can be fitted on the other worker. Total processing time is determined by the largest problem size.

First, we fit the five models iteratively without parallelization by using the \code{dc.fit} function of the \pkg{dclone} package:
<<eval=false>>=
t0 <- proc.time()
res3 <- dc.fit(dat, c("beta", "log.sigma"), glmm.model,
    n.clones = k, multiply = "n", unchanged = "np",
    n.update = n.update, n.iter = n.iter)
attr(res3, "timer") <- (proc.time() - t0)[3] / 60
@
Then, we fit the same models, using thesame arguments, but with parallelization (using size balancing and socket cluster with two workers):
<<eval=false>>=
cl <- makeCluster(2, type = "SOCK")
t0 <- proc.time()
res4 <- dc.parfit(cl, dat, c("beta", "log.sigma"), glmm.model,
    n.clones = k, multiply = "n", unchanged = "np",
    n.update = n.update, n.iter = n.iter)
attr(res4, "timer") <- (proc.time() - t0)[3] / 60
stopCluster(cl)
@

It took \Sexpr{round(attr(res3, "timer"), 2)} minutes without parallelization, as compared to 
\Sexpr{round(attr(res4, "timer"), 2)} minutes with parallelization 
(\Sexpr{round(100*attr(res4, "timer")/attr(res3, "timer"),2)} \%). The parallel processing time 
in this case was close to the time needed to fit the model with $k=20$ (\Sexpr{round(tab1[5,2], 2)} minutes), so it was really the larges problem that determined the processing time.

\section{Conclusions}

If the length of the vector $k$ (numbers of clones used) is short, or the number of available workers is limited to a few (2-4, i.e. the number of chains), the first type of parallelization (parallel MCMC chanis) and the the use of the \code{jags.parfit} function can be more efficient. If the length of $k$ is longer, or the sice of the computing cluster is high enough, the second type of parallelization (splitting the problem with respect to size) and the use of the \code{dc.parfit} function can be more efficient. Results presented in this paper can guide such decisions. But the advantage of parallelization was demonstrated in both cases: processing time decreased to \Sexpr{round(100*sum(pt2)/sum(pt1),2)} \% and round(100*attr(res4, "timer")/attr(res3, "timer"),2) \%, respectively.

\bibliography{dclone}

\end{document}
