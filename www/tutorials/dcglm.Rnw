\documentclass[article,shortnames,nojss]{jss}
\usepackage{thumbpdf}

%% need no \usepackage{Sweave.sty}
\SweaveOpts{engine = R, strip.white = TRUE, keep.source = TRUE, eps = FALSE}

\let\code=\texttt
\let\proglang=\textsf
%% \newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}}
\newcommand{\R}{\proglang{R}}
\newcommand{\class}[1]{`\code{#1}'}
\renewenvironment{Schunk}{\small}{}

%\VignetteIndexEntry{dcglm tutorial}
%\VignettePackage{dcglm}
%\VignetteDepends{dclone}
%\VignetteKeywords{Bayesian statistics, data cloning, maximum likelihood inference, generalized linear models, R}

\author{P\'eter S\'olymos\\University of Alberta}
\Plainauthor{P\'eter S\'olymos}

\title{Tutorial for the \pkg{dcglm} package}
\Plaintitle{Tutorial for the dcglm package}
\Shorttitle{Tutorial for the dcglm package}

\Abstract{
  This tutorial package to demonstrate the 
  capabilities of data cloning algorithm via 
  the infrastructure provided by the \pkg{dclone} package.
  Functions reproduce main features of the \code{glm}
  base function in \proglang{R} by using data cloning.
}

\Keywords{Bayesian statistics, data cloning, maximum likelihood inference, generalized linear models, \proglang{R}}
\Plainkeywords{Bayesian statistics, data cloning, maximum likelihood inference, generalized linear models, R}

\Address{
  P\'eter S\'olymos\\
  Alberta Biodiversity Monitoring Institute\\
  Department of Biological Sciences\\
  CW 405, Biological Sciences Bldg\\
  University of Alberta\\
  Edmonton, Alberta, T6G 2E9, Canada\\
  E-mail: \email{solymos@ualberta.ca}\\
}



\begin{document}


<<echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+   ", useFancyQuotes = FALSE, width = 76)
@


\section{Introduction}

Data cloning is a statistical computing method introduced by \citet{Lele2007}.
It exploits the computational simplicity of the Markov chain Monte Carlo (MCMC) algorithms
used in the Bayesian statistical framework, but it provides valid frequentist inferences such as the maximum 
likelihood estimates and their standard errors for complex hierarchical models.
The use of the data cloning algorithm is especially straightforward for complex models, 
where the number of unknowns increases with sample size (i.e.~mixed models),
because inference and prediction procedures are often hard to implement in such situations.

The \textbf{dclone} R package \citep{Solymos2009} aims to provide low level functionality to easily implement 
more specific higher level procedures based on data cloning for users familiar with the Bayesian methodology.
This tutorial, we develop write high level functions to duplicate the some features of 
the \code{glm} base function of \proglang{R} by using
the data cloning algorithm building on the infrastructure of the \pkg{dclone} package.

\section{Data generation}

We generate random data for Poisson and Binomal GLMs. First we define the number of locations (\code{n})
and the independent covariate (\code{x}). \code{X} represents the design matrix:
<<>>=
library(dclone)
set.seed(1234)
n <- 20
x <- runif(n, -1, 1)
X <- model.matrix(~x)
@
Parameters (\code{beta1}), linear predictor (\code{mu1}) 
and random response (\code{Y1}) for the Poisson case (log link function):
<<>>=
beta1 <- c(2, -1)
mu1 <- X %*% beta1
Y1 <- rpois(n, exp(mu1))
@
Parameters (\code{beta2}), linear predictor (\code{mu2}) 
and random response (\code{Y2}) for the Binomial (Bernoulli) case (logistic link function):
<<>>=
beta2 <- c(0, -1)
mu2 <- X %*% beta2
Y2 <- rbinom(n, 1, exp(mu2) / (1 + exp(mu2)))
@

\section{GLM based on the glm function}

Now we fit the Poisson and Binomail GLM by using the \code{glm} base function and inspect their summaries:
<<>>=
m1 <- glm(Y1 ~ x, family=poisson)
summary(m1)
m2 <- glm(Y2 ~ x, family=binomial)
summary(m2)
@

\section{The full Bayesian model for GLM and data cloning}

Here is the \proglang{JAGS} model for the Poisson case, with flat Normal priors for the regression coefficients:
<<>>=
glm.pois <- function() {
    for (i in 1:n) {
        Y[i] ~ dpois(lambda[i])
        log(lambda[i]) <- inprod(X[i,], beta[1,])
    }
    for (j in 1:np) {
        beta[1,j] ~ dnorm(0, 0.001)
    }
}
@
The data will be represented as a list (if we want to use data cloning consistently, 
we have to use the list format instead of the global environment):
<<>>=
dat1 <- list(n = length(Y1), Y = Y1, X = X, np = ncol(X))
str(dat1)
@
Now let's clone the data set (note that \code{n} should be multiplies, while
\code{np} must remain unchanged):
<<>>=
n.clones <- 5
dcdat1 <- dclone(dat1, n.clones, multiply = "n", unchanged = "np")
str(dcdat1)
@
To fit the model to the data with data cloning is as easy as this:
<<results=hide>>=
mod1 <- jags.fit(dcdat1, "beta", glm.pois)
@
<<>>=
summary(mod1)
@
Le's compare this \code{mcmc.list} (more accurately an \code{mcmc.list.dc}) object with the \code{glm} results:
<<>>=
cbind(true.values=beta1,
    glm.estimates=coef(m1), glm.se=summary(m1)$coefficients[,2],
    dc.estimates=coef(mod1), dc.se=dcsd(mod1))
@

Here is the \proglang{JAGS} model for the Binomial (Bernoulli, because \code{k}=1) case:
<<>>=
glm.bin <- function() {
    for (i in 1:n) {
        Y[i] ~ dbin(p[i], k)
        logit(p[i]) <- inprod(X[i,], beta[1,])
    }
    for (j in 1:np) {
        beta[1,j] ~ dnorm(0, 0.001)
    }
}
@
Putting together the data set is similar to the Poisson case:
<<>>=
dat2 <- list(n = length(Y2), Y = Y2, k = 1, X = X, np = ncol(X))
str(dat2)
@
but data cloning setup is a bit different. We don't have to repeat the data vectors
or columns in the design matrix \code{n.clones} times, because there is an easier way.
We multiply \code{Y} and \code{k} with \code{n.clones} and leave the other elements unchanged:
<<>>=
dcdat2 <- dclone(dat2, n.clones, multiply = c("Y","k"), unchanged = c("n", "np", "X"))
str(dcdat2)
@
Now fit the model to the data with data cloning:
<<results=hide>>=
mod2 <- jags.fit(dcdat2, "beta", glm.bin)
@
<<>>=
summary(mod2)
@
and compare results with \code{glm} results:
<<>>=
cbind(true.values=beta2,
    glm.estimates=coef(m2), glm.se=summary(m2)$coefficients[,2],
    dc.estimates=coef(mod2), dc.se=dcsd(mod2))
@

\section{The custommodel function}

The \code{custommodel} function enables us to resuse the same \proglang{JAGS} model
with minor modifications. For example we combine the above Poisson and Binomial model into one.
<<>>=
glm.model <- function() {
    for (i in 1:n) {
        Y[i] ~ dpois(lambda[i])
        Y[i] ~ dbin(p[i], k)
        log(lambda[i]) <- inprod(X[i,], beta[1,])
        logit(p[i]) <- inprod(X[i,], beta[1,])
    }
    for (j in 1:np) {
        beta[1,j] ~ dnorm(0, 0.001)
    }
}
@
If we want to use this in the \code{jags.fit} function, it would give the error message
about the attempt to define the nodes more than onece. To avoid this, we tell the function
which lines ahould be excluded:
<<>>=
custommodel(glm.model, c(4,6))
custommodel(glm.model, c(3,5))
@
so eventually we get back our original models. But these are not functions, but character vectors
of the class \class{custommodel}. \code{jags.fit} will recognize this.

Why do we want to complicate our lives with the \code{custommodel}?
Because \code{dpois}, \code{dbin}, and \code{inprod} are not recognised
as valid \proglang{R} objects or functions. So if our aim is to make an \proglang{R} package
that passes the rather strict \code{R CMD check}, so won't be published
at the Comprehensive \proglang{R} Archive Network (CRAN). A way to overcome this
situation is to define fake objects as e.g.~\code{inprod <- function() NULL},
but this option should be regaded as ugly and inefficient (unnecessary) as compared to 
a clean \code{custommodel} approach that will be presented in the next section.

\section{The main function dcglm}

Here our main function for the data cloning based estimating procedure
for the Poisson and Binomial GLMs:
<<>>=
dcglm <- function(formula, data = parent.frame(), family=c("poisson", "binomial"), n.clones=5, ...){
    glm.model <- c("model {",
                   "    for (i in 1:n) {",
                   "        Y[i] ~ dpois(lambda[i])",
                   "        Y[i] ~ dbin(p[i], k)",
                   "        log(lambda[i]) <- inprod(X[i,], beta[1,])",
                   "        logit(p[i]) <- inprod(X[i,], beta[1,])",
                   "    }",
                   "    for (j in 1:np) {",
                   "        beta[1,j] ~ dnorm(0, 0.001)",
                   "    }",
                   "}")
    family <- match.arg(family)
    lhs <- formula[[2]]
    formula.orig <- formula
    Y <- eval(lhs, data)
    formula[[2]] <- NULL
    rhs <- model.frame(formula, data)
    X <- model.matrix(attr(rhs, "terms"), rhs)
    dat <- list(n = length(Y), Y = Y, X = X, np = ncol(X), k = 1)
    if (family == "poisson") {
        model <- model <- custommodel(glm.model, c(4,6))
        dcdat <- dclone(dat, n.clones, multiply = "n", unchanged = "np")
    } else {
        model <- custommodel(glm.model, c(3,5))
        dcdat <- dclone(dat, n.clones, multiply = c("Y","k"), unchanged = c("n", "np", "X"))
    }
    mod <- jags.fit(dcdat, "beta", model, ...)
    COEF <- coef(mod)
    SE <- dcsd(mod)
    names(COEF) <- names(SE) <- colnames(X)
    mu <- X %*% COEF
    if (family == "poisson") {
        fitval <- drop(exp(mu))
        ll <- sum(log(fitval^Y * exp(-fitval)) - log(factorial(Y)))
    } else {
        fitval <- drop(exp(mu) / (1 + exp(mu)))
        ll <- sum(log(choose(1, Y) * fitval^Y * (1-fitval)^(1-Y)))
    }
    rval <- list(call=match.call(),
        mcmc = mod,
        y = Y,
        x = rhs, 
        model = X,
        fitted.values = fitval,
        linear.predictors = mu,
        formula = formula.orig,
        coefficients = COEF,
        std.error = SE,
        loglik = ll,
        family = family,
        df.residual = length(Y) - length(COEF),
        df.null = length(Y) - 1)
    class(rval) <- c("dcglm")
    rval
}
@

Let'g go through it step-by-step as pseudo-code:

\begin{enumerate}
  \item \code{glm.model} is the \code{custommodel} version of the \proglang{BUGS} model,
    unifying the Poisson and Binomial cases, as we have seen before.
  \item The \code{family} argument is recognized, and as a result, it can be given not
    only in full (e.g.~\code{family = "p"} is equivalent of \code{family = "poisson"}).
  \item \code{lhs} is the left-hand-side of the formula, \code{Y} is the value as a result
    of evaluating \code{lhs} in \code{data} (that is the parent frame, which is usually the global
    environment if not called from inside of a function).
  \item \code{formula[[2]] <- NULL} removes the left-hand-side from the formula.
  \item \code{rhs} is the right-hand-side, that is a model frame with variables
    defined in \code{data}.
  \item The design matrix \code{X} is a result of using the \code{"terms"} attribute
    of \code{rhs} and evaluated in \code{rhs}.
  \item \code{dat} is the Bayesian data representation.
  \item The \code{model} and the data cloned data representation
    (\code{dcdat}) depends on the \code{family} argument.
  \item \code{mod} is the fitted \code{mcmc.list} object. Dots (\code{...}) represents
    all the additional arguments that can be passed, including \code{n.update}, \code{n.iter},
    and \code{n.chains}.
  \item \code{COEF} is the \code{coef} method evaluated on the \code{mcmc.list} object \code{mod}.
    \code{SD} is the data cloned standard error (scaled by $\sqrt{k}$). 
    Names of \code{COEF} and \code{SD} follow column names of \code{X}.
  \item \code{mu} is the linear predictor (on log/logit scale), while \code{fitval} is the
    fitted value (response scale after using the appropriate inverse link function) and
    \code{ll} is the log-likelihood calculated from the probability mass function.
  \item \code{rval} is the return value, that is a list with elements commonly applied in
    objects representing model fit (cf.~for example element names with \code{names(m1)}):
    \begin{description}
        \item[call] the function call,
        \item[mcmc] the fitted \code{mcmc.list} object,
        \item[y] the response,
        \item[x] the model frame (right-hand-side), 
        \item[model] the design matrix,
        \item[fitted.values] fitted values,
        \item[linear.predictors] linear predictors,
        \item[formula] the formula argument of the call,
        \item[coefficients] means of the joint posterior distribution (maximum likelihood estimates),
        \item[std.error] standard errors of the MLE,
        \item[loglik] log-likelihood,
        \item[family] family argument of the call,
        \item[df.residual] residual degrees of freedom,
        \item[df.null] degrees of freedom in the null model.
    \end{description}
  \item Finally, we attach the class attribute and return \code{rval}.
\end{enumerate}

Fun, isn't it? See if it is actually working:
<<results=hide>>=
dcm1 <- dcglm(Y1 ~ x)
dcm2 <- dcglm(Y2 ~ x, family = "binomial")
@

If we are about to inspect these objects, well, it is a mess without some
additional helper functions. The most basic such functions (called methods in \proglang{R}
jargon) are \code{print} and \code{summary}. For our convenience, 
we also define some other methods, too. These are based on the so called S3
method dispatch system. That is, if a generic function is defined,
we can add class specific methods to it.

In our case, the most simple methods are the \code{coef} and \code{fitted},
because these only extract an element from the objects:
<<>>=
coef.dcglm <- function(object, ...) object$coefficients
fitted.dcglm <- function(object, ...) object$fitted.values
@
Compare with the \code{glm} results:
<<>>=
rbind(glm=coef(m1), dcglm=coef(dcm1))
rbind(glm=coef(m2), dcglm=coef(dcm2))
rbind(glm=fitted(m1), dcglm=fitted(dcm1))
rbind(glm=fitted(m2), dcglm=fitted(dcm2))
@

For the \code{logLik} method, it is necessary to follow the standard
rules, because AIC calculations depend on this method (this means, that we don't have to
define a method for AIC if the \code{logLik} method exists for a class):
<<>>=
logLik.dcglm <- function (object, ...)
    structure(object$loglik,
        df = object$df.null + 1 - object$df.residual,
        nobs = object$df.null + 1,
        class = "logLik")
@
Compare with the \code{glm} results:
<<>>=
logLik(m1)
logLik(dcm1)
logLik(m2)
logLik(dcm2)
AIC(m1, dcm1, m2, dcm2)
@

Now it is possible to write the \code{print} method:
<<>>=
print.dcglm <- function(x, digits = max(3, getOption("digits") - 3), ...) {
    cat("\nCall: ", deparse(x$call), "\n\n")
    cat("Coefficients:\n")
    print.default(format(x$coefficients, digits = digits), print.gap = 2, quote = FALSE)
    cat("\nDegrees of Freedom:", x$df.null, "Total (i.e. Null); ", x$df.residual, "Residual\n")
    cat("Log Likelihood:\t   ", format(signif(x$loglik, digits)), "\n")
    invisible(x)
}
@
Let's have a look at the resulting objects of our \code{dcglm} function:
<<>>=
dcm1
dcm2
@
Well done so far!

\section{Methods for inference}

The \code{summary} method returns the ML estimates, data cloning
standard errors, and Wald-type $z$ statistics and $p$-values:
<<>>=
summary.dcglm <- function(object, ...){
    COEF <- coef(object)
    SE <- object$std.error
    z <- COEF / SE
    p <-  2 * pnorm(-abs(z))
    stab <- cbind("Estimate" = COEF, "Std. Error" = SE,
        "z value" = z, "Pr(>|z|)" = p)
    rval <- list(call = object$call, 
        coefficients = stab, 
        loglik = object$loglik,
        df.residual = object$df.residual,
        df.null = object$df.null)
    class(rval) <- "summary.dcglm"
    rval
}
@
The return value here is also a list, repeating some of the elements of
the fitted \code{object}. To appropriately format the summary,
we use the \code{print} method for the object class \class{summary.dcglm}:
<<>>=
print.summary.dcglm <- 
function (x, digits = max(3, getOption("digits") - 3), 
    signif.stars = getOption("show.signif.stars"), ...) 
{
    cat("\nCall:\n")
    cat(paste(deparse(x$call), sep = "\n", collapse = "\n"), "\n", sep = "")
    cat("\nCoefficients:\n")
    printCoefmat(x$coefficients, digits = digits, signif.stars = signif.stars, na.print = "NA", ...)
    cat("\nDegrees of Freedom:", x$df.null, "Total (i.e. Null); ", x$df.residual, "Residual\n")
    cat("Log Likelihood:\t   ", format(signif(x$loglik, digits)), "\n")
    invisible(x)
}
@
Summaries of the \code{glm()} results and our models:
<<>>=
summary(m1)
summary(dcm1)
summary(m2)
summary(dcm2)
@
Piece of cake!

For the \code{confint}, we use the asymptotic normality result
of the data cloning theory \citep{Lele2007}, and the \code{confint} method defined for the
data cloned \code{mcmc.list} part of the fitted model object:
<<>>=
confint.dcglm <- function(object, parm, level = 0.95, ...) {
    rval <- confint(object$mcmc, parm, level, ...)
    rownames(rval) <- names(coef(object))
    rval
}
@
The 95\% confidence intervals for the model estimates are:
<<>>=
confint(m1)
confint(dcm1)
confint(m2)
confint(dcm2)
@
Differences are due to the fact, that \code{confint} for \code{glm} uses
profile likelihood, while \code{dcglm} confidence intervals are
based on the asymptotic normality assumption. Profile likelihood
can be computed based on data cloning \citep{Ponciano2009}
but that procedure is not covered here.

\section{Prediction based on the joint posterior distribution}

In the prediction, we use MCMC. The likelihood part of the 
\proglang{BUGS} model for the prediction is the same as for the
estimation. The only difference is in the prior specification:
<<>>=
glm.pred <- function() {
    for (i in 1:n) {
        Y[i] ~ dpois(z[i])
        Y[i] ~ dbin(z[i], k)
        log(z[i]) <- mu[i]
        logit(z[i]) <- mu[i]
        mu[i] <- inprod(X[i,], beta[1,])
    }
    beta[1,1:np] <- mvn[1:np]
    mvn[1:np] ~ dmnorm(coefs[], prec[,])
}
@
Note that we denote \code{lambda} or \code{p} as \code{z}, this will make life easier later.
We use again the \code{custommodel} approach to differentiate between the
Poisson and Binomial cases:
<<>>=
custommodel(glm.pred, c(4,6))
custommodel(glm.pred, c(3,5))
@
Let's consider the Poisson case only (the Binomial differs from it only by the specification of the
\code{model} argument based on the \code{custommodel} approach, and the fitted model used).
The prediction can be done by \code{jags.fit}, only the data specification
is somewhat different. We will define the model parameters based on the MLE (\code{coefs}) and the variance-covariance
matrix. We define a Multivariate Normal node for all the model parameters, by using the
inverse of the variance-covariance matrix as a precision matrix (\code{prec}). Be careful, the check for symmetry
in \proglang{JAGS} is stricter than the usual numerical precision in \proglang{R}, consequently we ensure that
this condition is met by using the \code{make.symmetric} function. The data specification will look like
(note, we are using the observed data in \code{X}, but algorithmically, this doesn't make any difference):
<<>>=
prec <- make.symmetric(solve(vcov(mod1)))
coefs <- coef(mod1)
prdat <- list(n = nrow(X), X = X, 
    np = ncol(X), k = 1, coefs = coefs, prec = prec)
@
We use the \code{jags.fit} function. One chain is usually enough:
<<results=hide,eval=false>>=
prval <- jags.fit(prdat, "z", custommodel(glm.pred, c(4,6)), n.chains = 1)
@
The resuling \code{mcmc.list} object contains the conditional posterior distribution for our
Poisson GLM based prediction with prediction intervals.

\section{Methods for prediction}

For our convenience, we can write a \code{vcov} method.
We simply use the \code{vcov} method defined for the \code{mcmc.list} part of 
the fitted model object and do some cosmetics on the names:
<<>>=
vcov.dcglm <- function(object, ...) {
    rval <- vcov(object$mcmc, ...)
    rownames(rval) <- colnames(rval) <- names(coef(object))
    rval
}
@
Comparison of the \code{glm} and \code{dcglm} approaches:
<<>>=
vcov(m1)
vcov(dcm1)
vcov(m2)
vcov(dcm2)
@
Quite similar as we expected.

The \code{predict} function will look like:
<<>>=
predict.dcglm <- function(object, newdata = NULL, type = c("link", "response"), se = FALSE, ...){
    glm.pred <- c("model {",
           "    for (i in 1:n) {",
           "        Y[i] ~ dpois(z[i])",
           "        Y[i] ~ dbin(z[i], k)",
           "        log(z[i]) <- mu[i]",
           "        logit(z[i]) <- mu[i]",
           "        mu[i] <- inprod(X[i,], beta[1,])",
           "    }",
           "    beta[1,1:np] <- mvn[1:np]",
           "    mvn[1:np] ~ dmnorm(coefs[], prec[,])",
           "    }")
    prec <- make.symmetric(solve(vcov(object)))
    coefs <- coef(object)
    if (is.null(newdata)) {
        X <- object$model
    } else {
        formul <- object$formula
        formul[[2]] <- NULL
        rhs <- model.frame(formul, newdata)
        X <- model.matrix(attr(rhs, "terms"), rhs)
    }
    type <- match.arg(type)
    params <- switch(type,
        "link" = "mu",
        "response" = "z")
    model <- switch(object$family,
        "poisson" = custommodel(glm.pred, c(4,6)),
        "binomial" = custommodel(glm.pred, c(3,5)))
    prdat <- list(n = nrow(X), X = X, 
        np = ncol(X), k = 1, coefs = coefs, prec = prec)
    prval <- jags.fit(prdat, params, model, ...)
    if (!se) {
        rval <- coef(prval)
    } else {
        rval <- list(fit = coef(prval), 
            se.fit = mcmcapply(prval, sd))
    }
    rval
}
@
The pseudo-code for \code{predict} is:
\begin{enumerate}
  \item \code{glm.predict} is the familiar \code{custommodel} specification.
  \item \code{prec} and \code{coefs} is needed for the data specification.
  \item If \code{newdata} is \code{NULL}, we use the extracted design matrix
    ou our fitted model (\code{object}). Else, we create the design matrix
    corresponding to our model from \code{newdata} (a data frame, containing the same
    covariates, but possibly with different values). For this extraction,
    we use the \code{formula} of the fitted model \code{object}.
  \item Based on the \code{type} argument, we will monitor (sample) the nodes
    \code{mu} (if \code{type = "link"}) or \code{z} (if \code{type = "response"}).
    \code{mu} corresponds to the values on the scale of the linear predictors,
    while \code{z} corresponds to the values on the response scale.
  \item \code{model} is determined by the \code{family} of the fitted model \code{object}.
  \item \code{prdat} is the data, \code{prmod} is the fitted MCMC object.
  \item If the \code{se} argument is \code{FALSE}, the return value will be the point 
    estimate vector of the prediction. If the \code{se} argument is \code{TRUE}, the
    return value will be a list including point estimates (\code{fit}) and standard
    errors (\code{se.fit}). Then, return thevalue.
\end{enumerate}

Now let's do the prediction for a range of \code{x} values from $-1$ to $1$ (call it \code{px}):
<<>>=
px <- data.frame(x=seq(-1, 1, len = 10))
px
@
The \code{glm} based predictions are:
<<>>=
pm1link <- predict(m1, newdata=px, type="link", se=TRUE)
pm1resp <- predict(m1, newdata=px, type="response", se=TRUE)
pm2link <- predict(m2, newdata=px, type="link", se=TRUE)
pm2resp <- predict(m2, newdata=px, type="response", se=TRUE)
@
The \code{dcglm} based predictions are:
<<results=hide>>=
pdcm1link <- predict(dcm1, newdata=px, type="link", se=TRUE)
pdcm1resp <- predict(dcm1, newdata=px, type="response", se=TRUE)
pdcm2link <- predict(dcm2, newdata=px, type="link", se=TRUE)
pdcm2resp <- predict(dcm2, newdata=px, type="response", se=TRUE)
@
Fig.~\ref{fig:pred} shows prediction results.
\begin{figure}[tb]
\begin{center}
<<predfig,echo=false,fig=true,width=7,height=7>>=
opar <- par(mfrow=c(2,2))
offs <- 0.02
se <- cbind(pm1link$fit - pm1link$se.fit, pm1link$fit + pm1link$se.fit)
plot(px$x-offs, pm1link$fit, col=2, ylim=range(se), xlim=range(px$x-offs, px$x+offs),
    main="Poisson GLM, predictor scale", xlab="linear predictor", ylab="log(lambda)")
errlines(px$x-offs, se, col=2)
se <- cbind(pdcm1link$fit - pdcm1link$se.fit, pdcm1link$fit + pdcm1link$se.fit)
points(px$x+offs, pdcm1link$fit, ylim=range(se), col=4)
errlines(px$x+offs, se, col=4)
legend("topright", col=c(2,4), lty=1, pch=21, legend=c("glm", "dcglm"))
se <- cbind(pm1resp$fit - pm1resp$se.fit, pm1resp$fit + pm1resp$se.fit)
plot(px$x-offs, pm1resp$fit, col=2, ylim=range(se), xlim=range(px$x-offs, px$x+offs),
    main="Poisson GLM, response scale", xlab="linear predictor", ylab="lambda")
errlines(px$x-offs, se, col=2)
se <- cbind(pdcm1resp$fit - pdcm1resp$se.fit, pdcm1resp$fit + pdcm1resp$se.fit)
points(px$x+offs, pdcm1resp$fit, ylim=range(se), col=4)
errlines(px$x+offs, se, col=4)
legend("topright", col=c(2,4), lty=1, pch=21, legend=c("glm", "dcglm"))
se <- cbind(pm2link$fit - pm2link$se.fit, pm2link$fit + pm2link$se.fit)
plot(px$x-offs, pm2link$fit, col=2, ylim=range(se), xlim=range(px$x-offs, px$x+offs),
    main="Binomial GLM, predictor scale", xlab="linear predictor", ylab="logit(p)")
errlines(px$x-offs, se, col=2)
se <- cbind(pdcm2link$fit - pdcm2link$se.fit, pdcm2link$fit + pdcm2link$se.fit)
points(px$x+offs, pdcm2link$fit, ylim=range(se), col=4)
errlines(px$x+offs, se, col=4)
legend("topright", col=c(2,4), lty=1, pch=21, legend=c("glm", "dcglm"))
se <- cbind(pm2resp$fit - pm2resp$se.fit, pm2resp$fit + pm2resp$se.fit)
plot(px$x-offs, pm2resp$fit, col=2, ylim=range(se), xlim=range(px$x-offs, px$x+offs),
    main="Binomial GLM, response scale", xlab="linear predictor", ylab="p")
errlines(px$x-offs, se, col=2)
se <- cbind(pdcm2resp$fit - pdcm2resp$se.fit, pdcm2resp$fit + pdcm2resp$se.fit)
points(px$x+offs, pdcm2resp$fit, ylim=range(se), col=4)
errlines(px$x+offs, se, col=4)
legend("topright", col=c(2,4), lty=1, pch=21, legend=c("glm", "dcglm"))
par(opar)
@
\end{center}
\caption{Prediction results based on the \code{glm} and \code{dcglm} approaches for the Poisson GLM.
  Points are prediction estimates, whiskers are prediction standard errors.}
  \label{fig:pred}
\end{figure}

\section{Making the dcglm package}

The easiest part now comes:
<<eval=false>>=
package.skeleton("dcglm", c("coef.dcglm","confint.dcglm","dcglm",
    "fitted.dcglm","logLik.dcglm","predict.dcglm",
    "print.dcglm","print.summary.dcglm","summary.dcglm","vcov.dcglm"))
@

Follow this workflow for your own model and estimating procedure, then edit the files (read the \emph{Writing \proglang{R} Extensions} manual) in the package directory, run \code{R CMD check}, and ditribute your package.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{dclone}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

