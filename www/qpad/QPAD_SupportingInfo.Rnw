\documentclass[11pt,letter]{report}
%\documentclass[11pt,letter]{scrreprt}
\usepackage[round,authoryear]{natbib}
\usepackage{hyperref}
\usepackage{graphicx,a4wide,color,ae,fancyvrb}
\usepackage[T1]{fontenc}

\let\code=\texttt
\let\proglang=\textsf
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}}
\newcommand{\R}{\proglang{R}}
\newcommand{\class}[1]{`\code{#1}'}

\title{Supporting Information:\\%
  \Large{Calibrating indices of avian density from 
  non-standardized survey data: making the most of a messy situation}}
\author{P\'{e}ter S\'{o}lymos\\%
  \href{mailto:solymos@ualberta.ca}{\code{solymos@ualberta.ca}}
  \and
  Steven M.~Matsuoka
  \and
  Erin M.~Bayne
  \and
  Subhash R.~Lele
  \and
  Patricia Fontaine
  \and
  Steven G.~Cumming
  \and
  Diana Stralberg
  \and
  Fiona K.~A.~Schmiegelow
  \and
  Samantha J.~Song}
\date{\today}

\begin{document}
%\SweaveOpts{concordance=TRUE}
\maketitle
\tableofcontents
<<settings,echo=FALSE,results= 'hide', message = FALSE>>=
options(width = 60)
opts_chunk$set(comment=NA, prompt=FALSE, highlight=TRUE,
  fig.path='figure/graphics-', cache.path='cache/graphics-', fig.align='center', 
  fig.width=5, fig.height=5, fig.show='hold', cache=TRUE, par=TRUE,
  size='small',
#  background=c(11,11,11),
  tidy.opts=list(keep.blank.line=TRUE, width.cutoff=60))
## delete these when final
#path <- "c:/Dropbox/bam/DApq3/ms/revision2/"
#setwd(path)
#source("BAM_QPAD_coefs_20130226.R")
#source("BAM_QPAD_functions_20130226.R")
#load_BAM_QPAD <- function(version) invisible(NULL)
@

\chapter{Introduction}

This document provides Supporting Information for the manuscript entitled
``Calibrating indices of avian density from 
  non-standardized survey data: making the most of a messy situation''
by \citep{solymos2013:qpad} (published in the journal
\emph{Methods in Ecology and Evolution}, 4:1047--1058).

The purpose of this document is to describe:
\begin{enumerate}
    \item how to estimate QPAD model parameters;
    \item how to retrieve the QPAD model parameter estimates reported in the paper;
    \item and how to use these estimates in statistical inference and prediction.
\end{enumerate}

\section{Software requirements}

\begin{itemize}
  \item \R\ for most of the calculations 
    (\citealp{Rcore}, downloadable from 
    \href{http://www.r-project.org/}{http://www.r-project.org/});
  \item \proglang{JAGS} for prediction interval calculations using Markov chain
    Monte Carlo (MCMC)
    (\citealp{JAGS}, downloadable from 
    \href{http://mcmc-jags.sourceforge.net/}{http://mcmc-jags.sourceforge.net/})
  \item \R\ extension packages: \pkg{detect} \citep{pkg-detect}, 
    \pkg{dcmle} \citep{Solymos2010} 
    (can be installed from \R\ console using the \code{install.packages} function).
\end{itemize}

\section{Files}

\begin{itemize}
  \item \code{BAM\_QPAD\_coefs\_20130226.R}: 
    \code{source}-able text file with estimated model parameters (available from
    \href{http://dcr.r-forge.r-project.org/qpad/BAM_QPAD_coefs_20130226.R}{%
    http://dcr.r-forge.r-project.org/\-qpad/\-BAM\_QPAD\_coefs\_20130226.R});
  \item \code{BAM\_QPAD\_functions\_20130226.R}: \code{source}-able text file with 
    \R\ functions for retrieving the information from the binary \R\ data file
    (available from 
    \href{http://dcr.r-forge.r-project.org/qpad/BAM_QPAD_functions_20130226.R}{%
    http://dcr.r-forge.r-project.org/\-qpad/\-BAM\_QPAD\_functions\_20130226.R}).
\end{itemize}

These files can be sourced by using the \code{load\_BAM\_QPAD()} function
of the \pkg{detect} \R\ package (version 0.3). This function allows the user to select
versions of the estimates interactively, this way updates to these
files are available upon request. Using \code{load\_BAM\_QPAD(version=1)}
loads the version used in this document.

\chapter{Conditional multinomial maximum likelihood estimation}

The estimation procedure described in the Appendix is implemented in the \code{cmulti}
function of the \pkg{detect} \R\ extension package. Input data specifications are 
described in the help page of the function (type \code{?cmulti} into \R\ console).
<<loadDetect,results='hide'>>=
library(detect)
@

\section{Input data}

Let us use 100 survey locations (\code{n}), and generate random values for the
covariate \code{x}:
<<rndData>>=
n <- 100
set.seed(1234)
x <- rnorm(n)
X <- cbind(1, x)
@
The function \code{simfun1} is used to simulate count data (value for
\code{tau} (Effective Detection Radius; EDR) is in units of 100 m:
<<sim1>>=
simfun1 <- function(n = 10, phi = 0.1, c=1, tau=0.8, type="rem") {
    if (type=="dis") {
        Dparts <- matrix(c(0.5, 1, NA,
                           0.5, 1, Inf,
                           1, Inf, NA), 3, 3, byrow=TRUE)
        D <- Dparts[sample.int(3, n, replace=TRUE),]
        CP <- 1-exp(-(D/tau)^2)
    } else {
        Dparts <- matrix(c(5, 10, NA,
                           3, 5, 10,
                           3, 5, NA), 3, 3, byrow=TRUE)
        D <- Dparts[sample.int(3, n, replace=TRUE),]
        CP <- 1-c*exp(-D*phi)
    }
    k <- ncol(D)
    P <- CP - cbind(0, CP[, -k, drop=FALSE])
    Psum <- rowSums(P, na.rm=TRUE)
    PPsum <- P / Psum
    Pok <- !is.na(PPsum)
    N <- rpois(n, 10)
    Y <- matrix(NA, ncol(PPsum), nrow(PPsum))
    Ypre <- sapply(1:n, function(i) rmultinom(1, N, PPsum[i,Pok[i,]]))
    Y[t(Pok)] <- unlist(Ypre)
    Y <- t(Y)
    list(Y=Y, D=D)
}
@
Now let us simulate counts under the removal model using constant
singing rate \code{phi}. The count matrix \code{Y} contains the
number of unique individuals first observed in time intervals defined in the
design matrix \code{D} which contains the enpoints of the corresponding
time intervals in minutes. 
Note that patterns in \code{NA} values must match between
the two matrices:
<<sim2>>=
vv <- simfun1(n=n, phi=exp(-1.5))
head(vv$Y)
head(vv$D)
@

\section{Removal sampling}

Estimation is done using the \code{cmulti} function.
The left hand side of the formula reads as \code{count | design}
where \code{count} is a matrix with cell counts, \code{design}
is the matrix describing the interval endpoints for the cells.
The right hand side is \code{1} because we use a constant model,
\code{type="rem"} stands for removal sampling:
<<sim3>>=
m1 <- cmulti(vv$Y | vv$D ~ 1, type="rem")
coef(m1)
@
When covariate \code{x} affects singing rate, the estimation is as follows:
<<sim4>>=
log.phi <- X %*% c(-2,-1) # log singing rate
vv <- simfun1(n=n, phi=exp(cbind(log.phi, log.phi, log.phi)))
m2 <- cmulti(vv$Y | vv$D ~ x, type="rem")
coef(m2)
@

\section{Distance sampling}

Simulation and estimation for the distance sampling model with half-normal
detection function is similar. We use constant \code{tau} parameter
(EDR, 100 m units, use\code{Inf} for unlimited distance):
<<sim5>>=
vv <- simfun1(n=n, tau=exp(-0.2), type="dis")
head(vv$Y)
head(vv$D)
m3 <- cmulti(vv$Y | vv$D ~ 1, type="dis")
coef(m3)
@
Effect of covariate \code{x} is estimated as:
<<sim6>>=
log.tau <- X %*% c(-0.5,-0.2) # log EDR
vv <- simfun1(n=n, tau=exp(cbind(log.tau, log.tau, log.tau)), type="dis")
m4 <- cmulti(vv$Y | vv$D ~ x, type="dis")
coef(m4)
@

\section{Convenience methods}

Several methods are defined for the fitted model objects to facilitate
statistical inference:
<<methods>>=
summary(m2)
summary(m4)
coef(m4)
vcov(m4)
AIC(m4)
confint(m4)
logLik(m4)
@

\section{Joint estimation for removal and distance sampling}

The following function simulates counts for the joint estimation of
singing rate and distance parameters:
<<sim7>>=
simfun12 <- function(n = 10, phi = 0.1, c=1, tau=0.8, type="rem") {
    Flat <- function(x, DIM, dur=TRUE) {
        x <- array(x, DIM)
        if (!dur) {
            x <- aperm(x,c(1,3,2))
        }
        dim(x) <- c(DIM[1], DIM[2]*DIM[3])
        x
    }
    Dparts1 <- matrix(c(5, 10, NA,
                        3, 5, 10,
                        3, 5, NA), 3, 3, byrow=TRUE)
    D1 <- Dparts1[sample.int(3, n, replace=TRUE),]
    CP1 <- 1-c*exp(-D1*phi)
    Dparts2 <- matrix(c(0.5, 1, NA,
                        0.5, 1, Inf,
                        1, Inf, NA), 3, 3, byrow=TRUE)
    D2 <- Dparts2[sample.int(3, n, replace=TRUE),]
    CP2 <- 1-exp(-(D2/tau)^2)
    k1 <- ncol(D1)
    k2 <- ncol(D2)
    DIM <- c(n, k1, k2)
    P1 <- CP1 - cbind(0, CP1[, -k1, drop=FALSE])
    P2 <- CP2 - cbind(0, CP2[, -k2, drop=FALSE])
    Psum1 <- rowSums(P1, na.rm=TRUE)
    Psum2 <- rowSums(P2, na.rm=TRUE)
    Pflat <- Flat(P1, DIM, dur=TRUE) * Flat(P2, DIM, dur=FALSE)
    PsumFlat <- Psum1 * Psum2
    PPsumFlat <- Pflat / PsumFlat
    PokFlat <- !is.na(PPsumFlat)
    N <- rpois(n, 10)
    Yflat <- matrix(NA, ncol(PPsumFlat), nrow(PPsumFlat))
    YpreFlat <- sapply(1:n, function(i) rmultinom(1, N, PPsumFlat[i,PokFlat[i,]]))
    Yflat[t(PokFlat)] <- unlist(YpreFlat)
    Yflat <- t(Yflat)
    Y <- array(Yflat, DIM)
    k1 <- dim(Y)[2]
    k2 <- dim(Y)[3]
    Y1 <- t(sapply(1:n, function(i) {
        count <- rowSums(Y[i,,], na.rm=TRUE)
        nas <- rowSums(is.na(Y[i,,]))
        count[nas == k2] <- NA
        count
    }))
    Y2 <- t(sapply(1:n, function(i) {
        count <- colSums(Y[i,,], na.rm=TRUE)
        nas <- colSums(is.na(Y[i,,]))
        count[nas == k2] <- NA
        count
    }))
    list(Y=Y, D1=D1, D2=D2, Y1=Y1, Y2=Y2)
}
@
Joint and independent estimation of constant singing rate and EDR:
<<sim8>>=
vv <- simfun12(n=n, phi=exp(-1.5), tau=exp(-0.2))
res <- cmulti2.fit(vv$Y, vv$D1, vv$D2)
res1 <- cmulti.fit(vv$Y1, vv$D1, NULL, "rem")
res2 <- cmulti.fit(vv$Y2, vv$D2, NULL, "dis")
@
Jointly and independently estimated
point estimates and standard errors are identical, the two
models are orthogonal (correlation is 0):
<<res1>>=
round(cbind(coef.joint=res$coef, 
    coef.indep=c(res1$coef, res2$coef)), 4)
round(cbind(SE.joint=sqrt(diag(res$vcov)), 
      SE.indep=c(sqrt(diag(res1$vcov)),sqrt(diag(res2$vcov)))), 4)
ifelse(cov2cor(res$vcov) < 10^-10, 0, cov2cor(res$vcov))
@

Joint and independent estimation of covariate specific singing rate and EDR:
<<sim9>>=
vv <- simfun12(n=n, 
               phi=exp(cbind(log.phi, log.phi, log.phi)),
               tau=exp(cbind(log.tau, log.tau, log.tau)))
res <- cmulti2.fit(vv$Y, vv$D1, vv$D2, X1=X, X2=X)
res1 <- cmulti.fit(vv$Y1, vv$D1, X, "rem")
res2 <- cmulti.fit(vv$Y2, vv$D2, X, "dis")
@
Jointly and independently estimated
point estimates and standard errors are identical, the two
models are orthogonal (correlation is 0):
<<res2>>=
round(cbind(coef.joint=res$coef, 
    coef.indep=c(res1$coef, res2$coef)), 4)
round(cbind(SE.joint=sqrt(diag(res$vcov)), 
      SE.indep=c(sqrt(diag(res1$vcov)),sqrt(diag(res2$vcov)))), 4)
round(ifelse(cov2cor(res$vcov) < 10^-10, 0, cov2cor(res$vcov)), 4)
@

\chapter{Retrieving QPAD estimates}

First we need to load necessary code and data after opening \R\
(commands can be copy-pasted from the document, \code{\#} marks comments):
<<loadQpad>>=
library(detect) # load detect package
# source estimates and functions
load_BAM_QPAD(version=1)
@
Print out the list of species acronyms (printing out a table linking 
acronyms to common and scientific names can be done by the \code{getBAMspeciestable}
function):
<<spplist>>=
getBAMspecieslist()
@
Print out the list of models used for estimating singing rates 
(\code{sra})
and effective detection radii (\code{edr}):
<<modlist>>=
getBAMmodellist()
@
Get version info:
<<getver>>=
getBAMversion()
@
The species acronym \code{OVEN} stands for 
Ovenbird (\emph{Seiurus aurocapilla}). 
This gives the estimated parameters for the Ovenbird from the 
models best supported by BIC:
<<summary>>=
summaryBAMspecies("OVEN")
@
The best supported model is returned by:
<<best>>=
bestmodelBAMspecies("OVEN", type="BIC")
@
The type argument can take values \code{"AIC"}, \code{"BIC"} or
\code{"multi"} (the latter returns model IDs randomly based on
model weights).

It is also possible to print out other model combinations 
(model IDs can be looked up from the \code{getBAMmodellist} function):
<<AICsumm>>=
summaryBAMspecies("OVEN", model.sra=8, model.edr=1)
@
To get all possible models compared, use this:
<<select>>=
selectmodelBAMspecies("OVEN")
@
The column \code{weights} indicates model weights
used by \code{bestmodelBAMspecies} with argument
\code{type="multi"}.


\chapter{Example data analysis}

\section{The Ovenbird data set}

We used a data set of Ovenbirds analyzed by 
\citet{Lele2011JPE} and by \citet{Solymos2012ENV}, available from the \pkg{detect}
\R\ extension package (data set name: \code{oven}). 
The Ovenbird study was conducted in Saskatchewan, Canada. 
Point counts were sampled according to the standards of the 
North American Breeding Bird Survey (3 minutes unlimited distance counts).

First, we calculate necessary covariates for the offsets. 
\code{JDAY} is Julian day in the unit range (divided by 365),
\code{TSSR} is calculated approximately in hours and divided by the 
the possible max (24):
<<ovenData>>=
oven$JDAY <- oven$julian / 365
oven$TSSR <- ((oven$timeday/8) - .75) / 24
oven$xlat <- as.numeric(scale(oven$lat)) # latitude is standardized
oven$xlong <- as.numeric(scale(oven$long)) # longitude is standardized
@
Introduce variables for protocol effects, duration (3 min) and point 
count radius (note that this has to be in 100 metres for 
density per ha, use \code{Inf} for unlimited distance):
<<ovenData2>>=
oven$dur <- 3
oven$dist <- Inf
@
The covariates for distance sampling are derived from the
proportion of forest and proportion of deciduous forest:
<<treeLcc>>=
pf <- oven$pforest
pd <- oven$pdecid
pc <- pf - pd
oven$LCC <- factor(5, levels=1:5)     # 5=OH open habitat
oven$LCC[pf > 0.25 & pc > pd]  <- "3" # 3=SC dense conifer
oven$LCC[pf > 0.25 & pc <= pd] <- "4" # 4=SD sparse deciduous
oven$LCC[pf > 0.6 & pc > pd]   <- "1" # 1=DC dense conifer
oven$LCC[pf > 0.6 & pc <= pd]  <- "2" # 2=DC dense deciduous
table(oven$LCC)
@

\subsection{QPAD offsets}

Here is how one can calculate the offsets based on the estimates 
without covariate effects:
<<bc0>>=
bc0 <- with(oven, globalBAMcorrections("OVEN", t=dur, r=dist))
summary(bc0)
@
The offsets based on possible covariate effects can be calculated as:
<<bc>>=
bm <- bestmodelBAMspecies("OVEN", type="BIC")
bc <- with(oven, localBAMcorrections("OVEN", t=dur, r=dist,
    jday=JDAY, tssr=TSSR, tree=pforest, lcc=LCC, 
    model.sra=bm$sra, model.edr=bm$edr))
summary(bc)
@


\section{Inference: point estimation}

\subsection{Poisson GLM} 

Here is the general way how one can specify the 
offsets and estimate density, for example here using Poisson generalized
linear model (GLM):
<<glm>>=
(mod <- glm(count ~ pforest + xlong, oven, family=poisson("log"), 
    offset=corrections2offset(bc)))
@
Such models are suitable for calculating point predictions for example for mapping density, but do not take into account the uncertainty associated with the singing rate and EDR estimates underlying the offsets:
<<glmX>>=
X <- model.matrix(~ pforest + xlong,
    data.frame(pforest=10:0/10, xlong=0))
summary(drop(exp(X %*% coef(mod))))
@

\subsection{Negative Binomial GLM} 

The Negative Binomial model is useful in case of overdispersion due to e.g.~missing covariate. The Negative Binomial GLM also uses the log link so the usual offset works. Care must be taken because the offset argument of the \code{glm.nb} function in the \pkg{MASS} library \citep{MASSbook} is not available. So the offset must be defined as part of the formula:
<<nb>>=
library(MASS)
(modNB <- glm.nb(count ~ pforest + xlong + 
    offset(corrections2offset(bc)), oven))
round(cbind(Pois=coef(mod), NegBin=coef(modNB)), 3)
@

\subsection{Poisson--Log-Normal mixed effects model} 

Poisson mixed effects models can be fitted for example via the \pkg{lme4} package 
\citep{pkg-lme4} using random intercept for routes:
<<lme4>>=
library(lme4)
mod4 <- glmer(count ~ pforest + xlong + (1 | route), oven,
    family="poisson", offset=corrections2offset(bc))
round(cbind(Pois=coef(mod), NegBin=coef(modNB), PLn=fixef(mod4)), 3)
@

\subsection{Logistic regression} 

For detection/non-detection situations, here is the suggested modification.
Note that probability of having 1 means probability of observing
non-zero ($>0$) counts within the sampling area.
using the complementary log--log link is related
to density from Poisson GLM with log link:
<<bin>>=
mod01 <- glm(ifelse(count > 0, 1, 0) ~ pforest + xlong,
    oven, family=binomial("cloglog"), 
    offset=corrections2offset(bc))
round(cbind(Pois=coef(mod), NegBin=coef(modNB), 
    PLn=fixef(mod4), Bin=coef(mod01)), 3)
@

\subsection{Zero-inflated count models} 

It is possible to use offsets in the count distribution of
zero-inflated models, such as zero-inflated Poisson
or zero-inflated Negative Binomial model as implemented
in the \code{zeroinfl} function of the \pkg{pscl} package
\citep{pkg-pscl-zeroinfl}:
<<zi>>=
library(pscl)
modZIP <- zeroinfl(count ~ pforest + xlong | 1,
    oven, dist="poisson", 
    offset=corrections2offset(bc))
modZINB <- zeroinfl(count ~ pforest + xlong | 1,
    oven, dist="negbin", 
    offset=corrections2offset(bc))
round(cbind(Pois=c(coef(mod), "ZI"=NA), NegBin=c(coef(modNB), NA), 
    PLn=c(fixef(mod4), NA), Bin=c(coef(mod01), NA),
    ZIP=coef(modZIP), ZINB=coef(modZINB)), 3)
@

\subsection{Classification and regression trees (CART)} 

This is how a CART model
can be specified using the offset approach using the \pkg{rpart} package
\citep{pkg-rpart}. Note that the specification
has the linear predictor for the Poisson rate on the response scale
($\lambda = f(x)$ when \code{method = "poisson"}, 
see package vignette), which needs to be
standardized by the correction, and not via offsets
(Fig.~\ref{fig:rpart}):
<<rpartpois>>=
library(rpart)
oven$C <- corrections(bc)
(cart <- rpart((count/C) ~ pforest + xlong,
    data=oven,
    method="poisson"))
@

Alternatively, one can use the \code{"anova"} with a log transformed
response variable.
Note that predicted values need to be back-transformed 
(Fig.~\ref{fig:rpart}):
<<rpartanova,fig.keep='none'>>=
(cart2 <- rpart(log(count+0.5) ~ pforest + xlong + offset(corrections2offset(bc)),
    data=oven,
    method="anova"))
opar <- par(mfrow=c(2,1), xpd=NA)
plot(cart)
text(cart)
title(main="poisson")
plot(cart2)
text(cart2)
title(main="anova")
par(opar)
@
\begin{figure}
<<rpartfig,fig.show='asis',echo=FALSE,fig.height=8>>=
opar <- par(mfrow=c(2,1), xpd=NA)
plot(cart)
text(cart, all=TRUE, cex=0.6)
title(main="poisson")
plot(cart2)
text(cart2, all=TRUE, cex=0.6)
title(main="anova")
par(opar)
@
\caption{Regression tree analysis of the Ovenbird 
data set using QPAD offsets.\label{fig:rpart}}
\end{figure}

The \pkg{rpart} based tree model has issues with handling
zero observations, therefore its use is not highly recommended for sparse counts.


\subsection{Boosted regression trees} 

This code snipped demonstrates how the offsets can
be specified using the \pkg{gbm} \R\ package \citep{pkg-gbm} (Fig.~\ref{fig:brtfig}). 
Of course one might want to
use more covariates in such cases, and more advanced settings for determining learning
rate e.g. through the \pkg{dismo} \R\ package \citep{pkg-dismo}.
Note the use of Poisson distribution, where the 
gradient function is calculated on the log scale (($\lambda = e^{f(x)}$), see package
vignette for specifications), therefore the use of the additive offset
is justified:
<<brt,fig.keep='none'>>=
library(gbm)
oven$off <- corrections2offset(bc)
(brt <- gbm(count ~ pforest + xlong + offset(off),
    data=oven,
    distribution="poisson"))
plot(brt)
@
\begin{figure}
<<brtfig,fig.show='asis',echo=FALSE>>=
plot(brt)
@
\caption{Influence plot for the proportion of forest
based on the boosted regression tree analysis of the Ovenbird 
data set using QPAD offsets.\label{fig:brtfig}}
\end{figure}

\subsection{Regularization approaches} 

This example shows the regularized Poisson
GLM using the elastic net penalty using the \pkg{glmnet} \R\ package
\citep{Friedman:Hastie:Tibshirani:2009:JSSOBK:v33i01} (Fig.~\ref{fig:enetfig}):
<<glmnet,fig.keep='none'>>=
library(glmnet)
enet <- glmnet(model.matrix(mod), mod$y, family="poisson",
    offset=corrections2offset(bc))
coef(enet)
plot(enet)
@
\begin{figure}
<<enetfig,fig.show='asis',echo=FALSE>>=
plot(enet)

@
\caption{Coefficent profile from regularized Poisson GLM 
analysis of the Ovenbird data set based on
QPAD offsets.\label{fig:enetfig}}
\end{figure}

\section{Inference: parameter uncertainty}

\subsection{Non-parametric bootstrap} 

Here is a simple implementation of the non-parametric
bootstrap for Poisson GLM using fixed offset.
The \code{hbootindex} function creates indices for the bootstrap
iterations by taking into account the grouped nature of the data set.
It resamples routes first, then resamples stops within the resampled routes.
<<boot0>>=
set.seed(1234)
Bi <- hbootindex(oven$route, strata=rep(1, nrow(oven)), B=199)
modB <- t(apply(Bi, 2, function(z) {
    bc <- with(oven, localBAMcorrections("OVEN", t=dur, r=dist,
        jday=JDAY, tssr=TSSR, tree=pforest, lcc=LCC,
        model.sra=bm$sra, model.edr=bm$edr))
    coef(glm(count ~ pforest + xlong, oven[z,], family="poisson", 
        offset=corrections2offset(bc)[z]))
}))
@
See how standard errors compare between Wald-type and bootstrap approach:
<<se1>>=
round(cbind(wald_fixed=sqrt(diag(vcov(mod))),
    boot_fixed=sqrt(diag(cov(modB)))), 3)
@

To incorporate 
uncertainty w.r.t.~singing rate and distance sampling parameter estimates, 
one can use nonparametric bootstrap to generate offsets. These offsets will 
represent the uncertainty in singing rate and distance sampling parameter 
estimates, so the error can be propagated through the GLM. 

For this, we can use the \code{boot} argument in the 
functions \code{globalBAMcorrections} and 
\code{localBAMcorrections} making the call \code{B} times:
<<boot1>>=
modB1 <- t(apply(Bi, 2, function(z) {
    bc <- with(oven, localBAMcorrections("OVEN", t=dur, r=dist,
        jday=JDAY, tssr=TSSR, tree=pforest, lcc=LCC,
        model.sra=5, model.edr=1,
        boot=TRUE))
    coef(glm(count ~ pforest + xlong, oven[z,], family="poisson", 
        offset=corrections2offset(bc)[z]))
}))
@
We can compare standard errors again:
<<se2>>=
round(cbind(wald_fixed=sqrt(diag(vcov(mod))),
    boot_fixed=sqrt(diag(cov(modB))),
    boot_boot=sqrt(diag(cov(modB1)))), 3)
@
The error in offsets can be propagated through other 
related count models similarly.

It is also possible to take into account model selection uncertainty
in the QPAD estimates by randomly choosing among the possible
singing rate and distance models based on model weights
(with and without the parametric bootstrap procedure
in the offsets switched by the \code{boot} argument):
<<boot2>>=
modB2 <- t(apply(Bi, 2, function(z) {
    mm <- bestmodelBAMspecies("OVEN", type="multi")
    bc <- with(oven, localBAMcorrections("OVEN", t=dur, r=dist,
        jday=JDAY, tssr=TSSR, tree=pforest, lcc=LCC, 
        model.sra=mm$sra, model.edr=mm$edr,
        boot=FALSE))
    coef(glm(count ~ pforest + xlong, oven[z,], family="poisson", 
        offset=corrections2offset(bc)[z]))
}))
modB3 <- t(apply(Bi, 2, function(z) {
    mm <- bestmodelBAMspecies("OVEN", type="multi")
    bc <- with(oven, localBAMcorrections("OVEN", t=dur, r=dist,
        jday=JDAY, tssr=TSSR, tree=pforest, lcc=LCC, 
        model.sra=mm$sra, model.edr=mm$edr,
        boot=TRUE))
    coef(glm(count ~ pforest + xlong, oven[z,], family="poisson", 
        offset=corrections2offset(bc)[z]))
}))
round(cbind(wald_fixed=sqrt(diag(vcov(mod))),
    boot_fixed=sqrt(diag(cov(modB))),
    boot_boot=sqrt(diag(cov(modB1))),
    boot_multi=sqrt(diag(cov(modB2))),
    boot_bmulti=sqrt(diag(cov(modB2)))), 3)
@
It is clear that the uncertainty in this case is driven by
parameter uncertainty, and not model uncertainty w.r.t.~the
offsets used.

\subsection{Bayesian and frequentist approach to hierarchical modeling}

In the next example we consider a Poisson-Log-Normal generalized linear mixed 
model with a random intercept for routes, which also incorporates 
detectability related uncertainty. 

This model uses a global maximization 
technique called data cloning \citep{Lele2007ELE,Lele2010JASA}, which takes 
advantage of Bayesian MCMC techniques for maximum likelihood estimation. 
The software implementation is described in \citet{Solymos2010}. 
Note that using a single clone is identical to the Bayesian hierarchical
modeling.
<<dcmle>>=
library(dcmle) # load dcmle package
load.module("glm") # load glm module for JAGS
model <- function() {
    for (i in 1:n) {
      Y[i] ~ dpois(lam[i])
      log(lam[i]) <- inprod(X[i,], beta) + E[gr[i]] + log(A[i] * p[i])
      p[i] <- 1 - exp(-3 * phi[i])
      A[i] <- 3.141593 * tau[i]^2
      log(phi[i]) <- inprod(Z1[i,], theta01)
      log(tau[i]) <- inprod(Z2[i,], theta02)
    }
    for (j in 1:m) {
      E[j] ~ dnorm(0, 1/exp(log.sigma)^2)
    }
    for (k in 1:np) {
      beta[k] ~ dnorm(pr[k], 1)
    }
    log.sigma ~ dnorm(-2, 0.01)
    theta01 ~ dmnorm(theta1, Sigma1)
    theta02 ~ dmnorm(theta2, Sigma2)
}
dat <- list(Y=oven$count, 
    X=model.matrix(~ pforest + xlong, oven),
    np=3, n=nrow(oven), m=length(unique(oven$route)),
    gr=dciid(as.integer(as.factor(oven$route))), 
    pr=coef(mod),
    theta1=coefBAMspecies("OVEN", bm$sra, bm$edr)$sra,
    Z1=model.matrix(~JDAY, oven),
    Sigma1=solve(vcovBAMspecies("OVEN", bm$sra, bm$edr)$sra),
    theta2=coefBAMspecies("OVEN", bm$sra, bm$edr)$edr,
    Z2=model.matrix(~LCC, oven),
    Sigma2=solve(vcovBAMspecies("OVEN", bm$sra, bm$edr)$edr))
dcf <- makeDcFit(model=model,
    data=dat,
    params=c("beta","log.sigma"),
    multiply=c("n","m"),
    unchanged=c("np","pr","theta1","theta2","Sigma1","Sigma2"))
cl <- makePSOCKcluster(3) # parallel computing for speed up
K <- c(1,2) # sequence for the number of clones to use
parLoadModule(cl, "glm") # load glm module for JAGS on workers
dcm <- dcmle(dcf, n.clones=K, n.update=2000,
    n.iter=2000, cl=cl, partype="parchains")
stopCluster(cl) # close cluster
summary(dcm)
@
The model reveals results similar to the GLM example above, 
Proportion of forest cover around points had a significant 
positive effect on density, while longitude had a negative 
non-significant effect. The estimate of the route level random 
effect was $\hat{\sigma}$ = \Sexpr{round(exp(coef(dcm)["log.sigma"]), 2)}
(SE \Sexpr{round(sd(exp(as.matrix(dcm)[,"log.sigma"])) * sqrt(nclones(dcm)), 2)}).

\section{Prediction}

Point predictions (unconditional on the observations) and associated prediction intervals can be calculated by the same MCMC technique:
<<pmodel,fig.keep='none'>>=
pmodel <- function() {
    for (i in 1:n) {
        Y[i] ~ dpois(lam[i])
        log(lam[i]) <- inprod(X[i,], tmp[1:np]) + E[i]
        E[i] ~ dnorm(0, 1/exp(tmp[np+1])^2)
    }
    tmp[1:(np+1)] ~ dmnorm(cf, Sig)
}
pf <- 10:0/10
ND <- expand.grid(pforest=pf,xlong=c(-1.8,0,1.8))
Xnew <- model.matrix(~ pforest + xlong, ND)
## grouping is random (so this is at region and not route level)
dat1 <- list(X=Xnew,
    np=3, n=nrow(Xnew), 
    cf=coef(dcm),
    Sig=solve(vcov(dcm)))
dcf2p1 <- makeDcFit(model=pmodel,
    data=dat1,
    params=c("lam"))
pm1 <- dcmle(dcf2p1, n.clones=1)
pm1 <- as.matrix(pm1)
ND$mean <- colMeans(pm1)
ND$cl1 <- apply(pm1, 2, quantile, probs=0.025)
ND$cl2 <- apply(pm1, 2, quantile, probs=0.975)
## drawing the figure
op <- par(las=1)
plot(mean ~ pforest, ND, col=as.integer(as.factor(ND$xlong)), 
    ylim=c(0, max(ND$cl2)), type="n",
    xlab="Proportion of forest", ylab="Density (males / ha)", axes=FALSE)
polygon(c(pf, rev(pf)), c(ND$cl1[ND$xlong>0], rev(ND$cl2[ND$xlong<0])),
    border=NA, col="grey")
polygon(c(pf, rev(pf)), c(ND$mean[ND$xlong>0], rev(ND$mean[ND$xlong<0])),
    border=NA, col="black")
lines(pf, ND$mean[ND$xlong==0], col="white", lwd=2)
box(bty="l")
axis(1, tck=0.02)
axis(2, tck=0.02)
par(op)
@
\begin{figure}
<<plot1,fig.show='asis',echo=FALSE>>=
op <- par(las=1)
plot(mean ~ pforest, ND, col=as.integer(as.factor(ND$xlong)), 
    ylim=c(0, max(ND$cl2)), type="n",
    xlab="Proportion of forest", ylab="Density (males / ha)", axes=FALSE)
polygon(c(pf, rev(pf)), c(ND$cl1[ND$xlong>0], rev(ND$cl2[ND$xlong<0])),
    border=NA, col="grey")
polygon(c(pf, rev(pf)), c(ND$mean[ND$xlong>0], rev(ND$mean[ND$xlong<0])),
    border=NA, col="black")
lines(pf, ND$mean[ND$xlong==0], col="white", lwd=2)
box(bty="l")
axis(1, tck=0.02)
axis(2, tck=0.02)
par(op)
@
\caption{Predicted density of Ovenbird as a function of forest cover, 
based on a generalized linear mixed model (GLMM) fitted to a 
data set from Saskatchewan, Canada. Grey shade represents 95\% 
prediction intervals incorporating uncertainties related to the 
random effect and the detectability corrections. 
Black shade indicates range of variation in mean predictions 
due to longitude, white line represents the prediction at 
the centroid of the study area.\label{fig:plot1}}
\end{figure}
Predicted density in areas with 100\% forest cover was 
1.05 males / ha ranging from 0.9 to 1.25 depending on longitude. 
This prediction almost equals the density estimate of 0.99 
(95\% confidence limits, 0.85-1.12) by Bayne (2000) 
based on territory mapping in the same region (Fig.~\ref{fig:plot1}). 

Point predictions conditional on the observations and associated prediction intervals given the observations can be calculated in a similar manner (Fig.~\ref{fig:plot2}).

<<dat2,fig.keep='none'>>=
dat2 <- list(Y=dat$Y, X=dat$X,
    np=3, n=nrow(dat$X),
    cf=coef(dcm),
    Sig=solve(vcov(dcm)))
dcf2p2 <- makeDcFit(model=pmodel,
    data=dat2,
    params=c("lam"))
pm2 <- dcmle(dcf2p2, n.clones=1, n.chains=1)
pm2 <- as.matrix(pm2)
pp <- data.frame(mean=colMeans(pm2),
    cl1=apply(pm1, 2, quantile, probs=0.025),
    cl2=apply(pm1, 2, quantile, probs=0.975))
summary(pp)
boxplot(pp$mean~dat$Y,
    xlab="Observed counts", ylab="Predicted mean density")
@
\begin{figure}
<<plot2,fig.show='asis',echo=FALSE>>=
boxplot(pp$mean~dat$Y,
    xlab="Observed counts", ylab="Predicted mean density")
@
\caption{The relationship between observed counts and
predicted mean density conditional on the observations
based on the Ovenbird data set (males / ha).\label{fig:plot2}}
\end{figure}

\chapter{Calculating offsets based on independent estimates}

Using the estimates derived from our extensive BAM data
can help in data deficient situations, but the QPAD approach
for using offsets allows the use of custom derived estimates.
The only requirement is that estimates need to be consistent
with the QPAD approach.

One can create a table with corrections using independently derived point
estimates for singing rates (\code{phi}) and distance parameter (\code{tau}):
<<custom1>>=
customBAMcorrections(r=rep(c(0.5, 1), 2), t=rep(c(5, 10), each=2),
    phi=seq(0.5, 0.8, len=4), tau=seq(0.5, 0.8, len=4))
@

Similarly, one can supply a vector of values for 
\code{phi} and \code{tau}:
<<custom2>>=
customBAMcorrections(r=rep(c(0.5, 1), 2), t=rep(c(5, 10), each=2),
    phi=seq(0.5, 0.8, len=4), tau=seq(0.5, 0.8, len=4))
@

Custom values can be derived from fitted models based on
the conditional likelihood estimating procedure:
<<custom3>>=
head(customBAMcorrections(r=rep(1, n), t=rep(10, n),
    phi=fitted(m2), tau=fitted(m4)))
@


\chapter{Further examples}

Further examples can be found at the BAM website 
(\href{http://www.borealbirds.ca}{http://www.borealbirds.ca}) 
under the 
\href{http://www.borealbirds.ca/avian_db/accounts.php}{Results tab}. 

These include spatial maps of expected mean abundance for 
Bird Conservation Regions (BCRs) within provinces and territories of Canada, and 
relative densities within various land cover classes per spatial units. 
Information is available for 70 bird species.

For example, the Ovenbird results 
(\href{http://www.borealbirds.ca/avian_db/accounts.php/Seiurus+aurocapilla/estimates}{%
density estimates}, 
\href{http://www.borealbirds.ca/avian_db/accounts.php/Seiurus+aurocapilla/habitat}{%
habitat associations}, and more) are available at the
\href{http://www.borealbirds.ca/avian_db/accounts.php/Seiurus+aurocapilla}{BAM website}.

\bibliographystyle{abbrvnat}
\bibliography{qpad}

\end{document}

%% Spell checking:
%% xx <- utils:::aspell("QPAD_SupportingInfo.Rnw", filter="Sweave")
%% xx$Suggestions <- sapply(xx$Suggestions, paste, collapse=" ")
%% xx <- as.data.frame(xx)
%% write.csv(xx, file="aspell.csv")

