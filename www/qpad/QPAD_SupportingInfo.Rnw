\documentclass[11pt,letter]{report}
\usepackage[round,authoryear]{natbib}
\usepackage{hyperref}
\usepackage{graphicx,a4wide,color,ae,fancyvrb}
\usepackage[T1]{fontenc}

\let\code=\texttt
\let\proglang=\textsf
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}}
\newcommand{\R}{\proglang{R}}
\newcommand{\class}[1]{`\code{#1}'}

\title{Supporting Information:\\%
  \Large{Calibrating indices of avian density from 
  non-standardized survey data: making the most of a messy situation}}
\author{P\'{e}ter S\'{o}lymos\\%
  \href{mailto:solymos@ualberta.ca}{\code{solymos@ualberta.ca}}
  \and
  Steven M.~Matsuoka
  \and
  Erin M.~Bayne
  \and
  Subhash R.~Lele
  \and
  Patricia Fontaine
  \and
  Steven G.~Cumming
  \and
  Diana Stralberg
  \and
  Fiona K.~A.~Schmiegelow
  \and
  Samantha J.~Song}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
<<settings,echo=FALSE,results= 'hide', message = FALSE>>=
options(width = 60)
opts_chunk$set(comment=NA, prompt=FALSE, highlight=TRUE,
  fig.path='figure/graphics-', cache.path='cache/graphics-', fig.align='center', 
  fig.width=5, fig.height=5, fig.show='hold', cache=TRUE, par=TRUE,
  size='small',
#  background=c(11,11,11),
  tidy.opts=list(keep.blank.line=TRUE, width.cutoff=60))
path <- "c:/Dropbox/bam/DApq3/ms/revision/"
@

\chapter{Introduction}

This document provides Supporting Information for the manuscript entitled
``Calibrating indices of avian density from 
  non-standardized survey data: making the most of a messy situation''
by S\'{o}lymos et al. submitted to the journal
\emph{Methods in Ecology and Evolution} (ID MEE-12-11-472).

The purpose of this document is to describe:
\begin{enumerate}
    \item how to estimate QPAD model parameters;
    \item how to retrieve the QPAD model parameter estimates reported in the paper;
    \item and how to use these estimates in statistical inference and prediction.
\end{enumerate}

\section{Software requirements}

\begin{itemize}
  \item \R\ for most of the calculations 
    (\citep{Rcore}, downloadable from 
    \href{http://www.r-project.org/}{http://www.r-project.org/});
  \item \proglang{JAGS} for prediction interval calculations using Markov chain
    Monte Carlo (MCMC)
    (\citealp{JAGS}, downloadable from 
    \href{http://mcmc-jags.sourceforge.net/}{http://mcmc-jags.sourceforge.net/})
  \item \R\ extension packages: \pkg{detect} \citep{pkg-detect}, 
    \pkg{dcmle} \citep{Solymos2010} 
    (can be installed from \R\ console using the \code{install.packages} function).
\end{itemize}

\section{Files}

\begin{itemize}
  \item \code{BAMCOEFS\_QPADpaper\_20130226.R}: 
    sourceable text file with estimated model parameters (available from
    \href{http://dcr.r-forge.r-project.org/qpad/BAMCOEFS_QPADpaper_20130226.rda}{%
    http://dcr.r-forge.r-project.org/\-qpad/\-BAMCOEFS\_QPADpaper\_20130226.rda});
  \item \code{BAMcorrections\_20130226.R}: sourceable text file with 
    \R\ functions for retrieving the information from the binary \R\ data file
    (available from 
    \href{http://dcr.r-forge.r-project.org/qpad/BAMcorrections_20130226.R}{%
    http://dcr.r-forge.r-project.org/\-qpad/\-BAMcorrections\_20130226.R}).
\end{itemize}

\chapter{Conditional multinomial maximum likelihood estimation}

The estimation procedure described in the Appendix is implemented in the \code{cmulti}
function of the \pkg{detect} \R\ extension package. Input data specifications are described in the help page of the function (type \code{?cmulti} into \R\ console).
<<>>=
library(detect)
@

\section{Input data}

Let us use 100 survey locations (\code{n}), and generate random values for the
covariate \code{x}:
<<>>=
n <- 100
x <- rnorm(n)
X <- cbind(1, x)
@
The function \code{simfun1} is used to simulate count data (value for
\code{tau} (Effective Detection Radius; EDR) is in units of 100 m:
<<cache=TRUE>>=
simfun1 <- function(n = 10, phi = 0.1, c=1, tau=0.8, type="rem") {
    if (type=="dis") {
        Dparts <- matrix(c(0.5, 1, NA,
                           0.5, 1, Inf,
                           1, Inf, NA), 3, 3, byrow=TRUE)
        D <- Dparts[sample.int(3, n, replace=TRUE),]
        CP <- 1-exp(-(D/tau)^2)
    } else {
        Dparts <- matrix(c(5, 10, NA,
                           3, 5, 10,
                           3, 5, NA), 3, 3, byrow=TRUE)
        D <- Dparts[sample.int(3, n, replace=TRUE),]
        CP <- 1-c*exp(-D*phi)
    }
    k <- ncol(D)
    P <- CP - cbind(0, CP[, -k, drop=FALSE])
    Psum <- rowSums(P, na.rm=TRUE)
    PPsum <- P / Psum
    Pok <- !is.na(PPsum)
    N <- rpois(n, 10)
    Y <- matrix(NA, ncol(PPsum), nrow(PPsum))
    Ypre <- sapply(1:n, function(i) rmultinom(1, N, PPsum[i,Pok[i,]]))
    Y[t(Pok)] <- unlist(Ypre)
    Y <- t(Y)
    list(Y=Y, D=D)
}
@
Now let us simulate counts under the removal model using constant
singing rate \code{phi}. The count matrix \code{Y} contains the
number of unique individuals first observed in time intervals defined in the
design matrix \code{D} which contains the enpoints of the corresponding
time intervals in minutes. 
Note that patterns in \code{NA} values muxt match between
the two matrices:
<<cache=TRUE>>=
vv <- simfun1(n=n, phi=exp(-1.5))
head(vv$Y)
head(vv$D)
@

\section{Removal sampling}

Estimation is done using the \code{cmulti} function.
The left hand side of the formula reads as \code{count | design}
where \code{count} is a matrix with cell counts, \code{design}
is the matrix describing the interval endpoints for the cells.
The right hand side is \code{1} because we use a constant model,
\code{type="rem"} stands for removal sampling:
<<cache=TRUE>>=
m1 <- cmulti(vv$Y | vv$D ~ 1, type="rem")
coef(m1)
@
When covariate \code{x} affects singing rate, the estimation is as follows:
<<cache=TRUE>>=
log.phi <- X %*% c(-2,-1) # log singing rate
vv <- simfun1(n=n, phi=exp(cbind(log.phi, log.phi, log.phi)))
m2 <- cmulti(vv$Y | vv$D ~ x, type="rem")
coef(m2)
@

\section{Distance sampling}

Simulation and estimation for the distance sampling model with half-normal
detection function is similar. We use constant \code{tau} parameter
(EDR, 100 m units, use\code{Inf} for unlimited distance):
<<cache=TRUE>>=
vv <- simfun1(n=n, tau=exp(-0.2), type="dis")
head(vv$Y)
head(vv$D)
m3 <- cmulti(vv$Y | vv$D ~ 1, type="dis")
coef(m3)
@
Effect of covariate \code{x} is estimated as:
<<cache=TRUE>>=
log.tau <- X %*% c(-0.5,-0.2) # log EDR
vv <- simfun1(n=n, tau=exp(cbind(log.tau, log.tau, log.tau)), type="dis")
m4 <- cmulti(vv$Y | vv$D ~ x, type="dis")
coef(m4)
@

\section{Convenience methods}

Several methods are defined for the fitted model objects to facilitate
statistical inference:
<<>>=
summary(m2)
summary(m4)
coef(m4)
vcov(m4)
AIC(m4)
confint(m4)
logLik(m4)
@

\section{Joint estimation for removal and distance sampling}

The following function simulates counts for the joint estimation of
singing rate and distance parameters:
<<cache=TRUE>>=
simfun12 <- function(n = 10, phi = 0.1, c=1, tau=0.8, type="rem") {
    Flat <- function(x, DIM, dur=TRUE) {
        x <- array(x, DIM)
        if (!dur) {
            x <- aperm(x,c(1,3,2))
        }
        dim(x) <- c(DIM[1], DIM[2]*DIM[3])
        x
    }
    Dparts1 <- matrix(c(5, 10, NA,
                        3, 5, 10,
                        3, 5, NA), 3, 3, byrow=TRUE)
    D1 <- Dparts1[sample.int(3, n, replace=TRUE),]
    CP1 <- 1-c*exp(-D1*phi)
    Dparts2 <- matrix(c(0.5, 1, NA,
                        0.5, 1, Inf,
                        1, Inf, NA), 3, 3, byrow=TRUE)
    D2 <- Dparts2[sample.int(3, n, replace=TRUE),]
    CP2 <- 1-exp(-(D2/tau)^2)
    k1 <- ncol(D1)
    k2 <- ncol(D2)
    DIM <- c(n, k1, k2)
    P1 <- CP1 - cbind(0, CP1[, -k1, drop=FALSE])
    P2 <- CP2 - cbind(0, CP2[, -k2, drop=FALSE])
    Psum1 <- rowSums(P1, na.rm=TRUE)
    Psum2 <- rowSums(P2, na.rm=TRUE)
    Pflat <- Flat(P1, DIM, dur=TRUE) * Flat(P2, DIM, dur=FALSE)
    PsumFlat <- Psum1 * Psum2
    PPsumFlat <- Pflat / PsumFlat
    PokFlat <- !is.na(PPsumFlat)
    N <- rpois(n, 10)
    Yflat <- matrix(NA, ncol(PPsumFlat), nrow(PPsumFlat))
    YpreFlat <- sapply(1:n, function(i) rmultinom(1, N, PPsumFlat[i,PokFlat[i,]]))
    Yflat[t(PokFlat)] <- unlist(YpreFlat)
    Yflat <- t(Yflat)
    Y <- array(Yflat, DIM)
    k1 <- dim(Y)[2]
    k2 <- dim(Y)[3]
    Y1 <- t(sapply(1:n, function(i) {
        count <- rowSums(Y[i,,], na.rm=TRUE)
        nas <- rowSums(is.na(Y[i,,]))
        count[nas == k2] <- NA
        count
    }))
    Y2 <- t(sapply(1:n, function(i) {
        count <- colSums(Y[i,,], na.rm=TRUE)
        nas <- colSums(is.na(Y[i,,]))
        count[nas == k2] <- NA
        count
    }))
    list(Y=Y, D1=D1, D2=D2, Y1=Y1, Y2=Y2)
}
@
Joint and independent estimation of constant singing rate and EDR:
<<cache=TRUE>>=
vv <- simfun12(n=n, phi=exp(-1.5), tau=exp(-0.2))
res <- cmulti2.fit(vv$Y, vv$D1, vv$D2)
res1 <- cmulti.fit(vv$Y1, vv$D1, NULL, "rem")
res2 <- cmulti.fit(vv$Y2, vv$D2, NULL, "dis")
@
Jointly and independently estimated
points estimates and standard errors are identical, the two
models are orthogonal (correlation is 0):
<<>>=
round(cbind(coef.joint=res$coef, 
    coef.indep=c(res1$coef, res2$coef)), 4)
round(cbind(SE.joint=sqrt(diag(res$vcov)), 
      SE.indep=c(sqrt(diag(res1$vcov)),sqrt(diag(res2$vcov)))), 4)
ifelse(cov2cor(res$vcov) < 10^-10, 0, cov2cor(res$vcov))
@

Joint and independent estimation of covariate specific singing rate and EDR:
<<cache=TRUE>>=
vv <- simfun12(n=n, 
               phi=exp(cbind(log.phi, log.phi, log.phi)),
               tau=exp(cbind(log.tau, log.tau, log.tau)))
res <- cmulti2.fit(vv$Y, vv$D1, vv$D2, X1=X, X2=X)
res1 <- cmulti.fit(vv$Y1, vv$D1, X, "rem")
res2 <- cmulti.fit(vv$Y2, vv$D2, X, "dis")
@
Jointly and independently estimated
points estimates and standard errors are identical, the two
models are orthogonal (correlation is 0):
<<>>=
round(cbind(coef.joint=res$coef, 
    coef.indep=c(res1$coef, res2$coef)), 4)
round(cbind(SE.joint=sqrt(diag(res$vcov)), 
      SE.indep=c(sqrt(diag(res1$vcov)),sqrt(diag(res2$vcov)))), 4)
round(ifelse(cov2cor(res$vcov) < 10^-10, 0, cov2cor(res$vcov)), 4)
@

\chapter{Retrieving QPAD estimates}

First we need to load necessary code and data after opening \R\
(commands can be copy-pasted from the document, \code{\#} marks comments):
<<>>=
library(detect) # load detect package
# source estimates
source("http://dcr.r-forge.r-project.org/qpad/BAMCOEFS_QPADpaper_20130226.R")
# source functions
source("http://dcr.r-forge.r-project.org/qpad/BAMcorrections_20130226.R")
@
Print out the list of species acronyms (printing out a table linking 
acronyms to common and scientific names can be done by the \code{getBAMspeciestable}
function):
<<>>=
getBAMspecieslist()
@
Print out the list of models used for estimating singing rates 
(\code{sra})
and effective detection radii (\code{edr}):
<<>>=
getBAMmodellist()
@
Get version info:
<<>>=
getBAMversion()
@
The species acronym \code{OVEN} stands for 
Ovenbird (\emph{Seiurus aurocapilla}). 
This gives the estimated parameters for the Ovenbird from the 
models best supported by BIC:
<<>>=
summaryBAMspecies("OVEN")
@
The best supported model is returned by:
<<>>=
bestmodelBAMspecies("OVEN")
@
It is also possible to print out other model combinations 
(model IDs can be looked up from the \code{getBAMmodellist} function):
<<>>=
summaryBAMspecies("OVEN", model.sra=8, model.edr=1)
@
To get all possible models compared, use this:
<<>>=
selectmodelBAMspecies("OVEN")
@


\chapter{Example data analysis}

\section{The Ovenbird data set}

We used a data set of Ovenbirds analyzed by 
\citet{Lele2011JPE} and by \citet{Solymos2012ENV}, available from the \pkg{detect}
\R\ extension package (data set name: \code{oven}). 
The Ovenbird study was conducted in Saskatchewan, Canada. 
Point counts were sampled according to the standards of the 
North American Breeding Bird Survey (3 minutes unlimited distance counts).

First, we calculate necessary covariates for the offsets. 
\code{JDAY} is Julian day in the unit range (divided by 365),
\code{TSSR} is calculated approximately in hours and divided by the max (24):
<<>>=
oven$JDAY <- oven$julian / 365
oven$TSSR <- ((oven$timeday/8) - .75) / 24
oven$xlat <- drop(scale(oven$lat)) # latitude is standardized
oven$xlong <- drop(scale(oven$long)) # longitude is standardized
@
Introduce variables for protocol effects, duration (3 min) and point 
count radius (note that this has to be in 100 metres for 
density per ha, use \code{Inf} for unlimited distance):
<<>>=
oven$dur <- 3
oven$dist <- Inf
@

\subsection{QPAD offsets}

Here is how one can calculate the offsets based on the estimates 
without covariate effects:
<<>>=
bc0 <- with(oven, globalBAMcorrections("OVEN", t=dur, r=dist))
head(bc0)
@
The offsets based on possible covariate effects can be calculated as:
<<>>=
bc <- with(oven, localBAMcorrections("OVEN", t=dur, r=dist,
    jday=JDAY, tssr=TSSR, tree=pforest, model.sra=5, model.edr=1))
head(bc)
@


\section{Inference: point estimation}

\subsection{Poisson GLM} 

Here is the general way how one can specify the 
offsets and estimate density, for example here using Poisson generalized
linear model (GLM):
<<>>=
(mod <- glm(count ~ pforest + xlong, oven, family=poisson("log"), 
    offset=corrections2offset(bc)))
@
Such models are suitable for calculating point predictions for example for mapping density, but do not take into account the uncertainty associated with the singing rate and EDR estimates underlying the offsets:
<<>>=
X <- model.matrix(~ pforest + xlong,
    data.frame(pforest=10:0/10, xlong=0))
summary(drop(exp(X %*% coef(mod))))
@

\subsection{Negative Binomial GLM} 

The Negative Binomial model is useful in case of overdispersion due to e.g.~missing covariate. The Negative Binomial GLM also uses the log link so the usual offset works. Care must be taken because the offset argument of the \code{glm.nb} function in the \pkg{MASS} library \citep{MASSbook} is not available. So the offset must be defined as part of the formula:
<<>>=
library(MASS)
(modNB <- glm.nb(count ~ pforest + xlong + 
    offset(corrections2offset(bc)), oven))
cbind(Pois=coef(mod), NegBin=coef(modNB))
@

\subsection{Poisson--Lognormal mixed effects model} 

Poisson mixed effects models can be fitted for example via the \pkg{lme4} package 
\citep{pkg-lme4} using random intercept for routes:
<<>>=
library(lme4)
mod4 <- glmer(count ~ pforest + xlong + (1 | route), oven,
    family="poisson", offset=corrections2offset(bc))
cbind(Pois=coef(mod), NegBin=coef(modNB), PLn=fixef(mod4))
@

\subsection{Logistic regression} 

For detection/non-detection situations, here is the suggested modification.
Note that probability of having 1 means probability of observing
non-zero ($>0$) counts within the sampling area.
using the complementary log--log link is related
to density from Poisson GLM with log link:
<<>>=
mod01 <- glm(ifelse(count > 0, 1, 0) ~ pforest + xlong,
    oven, family=binomial("cloglog"), 
    offset=corrections2offset(bc))
cbind(Pois=coef(mod), NegBin=coef(modNB), 
    PLn=fixef(mod4), Bin=coef(mod01))
@

\subsection{Classification and regression trees (CART)} 

This is how a CART model
can be specified using the offset approach using the \pkg{rpart} package
\citep{pkg-rpart} (Fig.~\ref{fig:rpart}):
<<fig.keep='none'>>=
library(rpart)
(cart <- rpart(count ~ pforest + xlong + offset(corrections2offset(bc)),
    data=oven,
    method="poisson"))
opar <- par(xpd=NA)
plot(cart)
text(cart)
par(opar)
@
\begin{figure}
<<rpartfig,fig.show='asis',echo=FALSE>>=
opar <- par(xpd=NA)
plot(cart)
text(cart)
par(opar)
@
\caption{Regression tree analysis of the Ovenbird 
data set using QPAD offsets.\label{fig:rpart}}
\end{figure}

\subsection{Boosted regression trees} 

This code snipped demonstrates how the offsets can
be specified using the \pkg{gbm} \R\ package \citep{pkg-gbm} (Fig.~\ref{fig:brtfig}). 
Of course one might want to
use more covariates in such cases, and more advanced settings for determining learning
rate e.g. through the \pkg{dismo} \R\ package \citep{pkg-dismo}:
<<fig.keep='none'>>=
library(gbm)
oven$off <- corrections2offset(bc)
(brt <- gbm(count ~ pforest + xlong + offset(off),
    data=oven,
    distribution="poisson"))
plot(brt)
@
\begin{figure}
<<brtfig,fig.show='asis',echo=FALSE>>=
plot(brt)
@
\caption{Influence plot for the proportion of forest
based on the boosted regression tree analysis of the Ovenbird 
data set using QPAD offsets.\label{fig:brtfig}}
\end{figure}

\subsection{Regularization approaches} 

This example shows the regularized Poisson
GLM using the elastic net penalty using the \pkg{glmnet} \R\ package
\citep{Friedman:Hastie:Tibshirani:2009:JSSOBK:v33i01} (Fig.~\ref{fig:enetfig}):
<<fig.keep='none'>>=
library(glmnet)
enet <- glmnet(model.matrix(mod), mod$y, family="poisson",
    offset=corrections2offset(bc))
coef(enet)
plot(enet)
@
\begin{figure}
<<enetfig,fig.show='asis',echo=FALSE>>=
plot(enet)

@
\caption{Coefficent profile from regularized Poisson GLM 
analysis of the Ovenbird data set based on
QPAD offsets.\label{fig:enetfig}}
\end{figure}

\section{Inference: parameter uncertainty}

\subsection{Non-parametric bootstrap} 

To incorporate 
uncertainty w.r.t.~singing rate and distance sampling parameter estimates, 
one can use nonparametric bootstrap to generate offsets. These offsets will 
represent the uncertainty in singing rate and distance sampling parameter 
estimates, so the error can be propagated through the GLM. 

For this, we can use the \code{boot} argument in the 
functions \code{globalBAMcorrections} and 
\code{localBAMcorrections} making the call \code{B} times:
<<cache=TRUE>>=
B <- 199
modB <- t(sapply(1:B, function(i) {
    bc <- with(oven, localBAMcorrections("OVEN", t=dur, r=dist,
        jday=JDAY, tssr=TSSR, tree=pforest, model.sra=5, model.edr=1,
        boot=TRUE))
    i <- sample.int(nrow(oven), nrow(oven), replace=TRUE)
    coef(glm(count ~ pforest + xlong, oven[i,], family="poisson", 
        offset=corrections2offset(bc)[i]))
}))
@
We can compare standard errors:
<<>>=
cbind(fixed_offset=sqrt(diag(vcov(mod))),
    boot_offset=sqrt(diag(cov(modB))))
@
The error in offsets can be propagated through other related count models similarly.

\subsection{Bayesian and frequentist approach to hierarchical modeling}

In the next example we consider a Poisson-Lognormal generalized linear mixed 
model with a random intercept for routes, which also incorporates 
detectability related uncertainty. 

This model uses a global maximization 
technique called data cloning \citep{Lele2007ELE,Lele2010JASA}, which takes 
advantage of Bayesian MCMC techniques for maximum likelihood estimation. 
The software implementation is described in \citet{Solymos2010}. 
Note that using a single clone is identical to the Bayesian hierarchical
modeling.
<<cache=TRUE>>=
library(dcmle) # load dcmle package
load.module("glm") # load glm module for JAGS
model <- function() {
    for (i in 1:n) {
      Y[i] ~ dpois(lam[i])
      log(lam[i]) <- inprod(X[i,], beta) + E[gr[i]] + log(A[i] * p[i])
      p[i] <- 1 - exp(-3 * phi[i])
      A[i] <- 3.141593 * tau[i]^2
      log(phi[i]) <- inprod(Z1[i,], theta01)
      log(tau[i]) <- inprod(Z2[i,], theta02)
    }
    for (j in 1:m) {
      E[j] ~ dnorm(0, 1/exp(log.sigma)^2)
    }
    for (k in 1:np) {
      beta[k] ~ dnorm(pr[k], 1)
    }
    log.sigma ~ dnorm(-2, 0.01)
    theta01 ~ dmnorm(theta1, Sigma1)
    theta02 ~ dmnorm(theta2, Sigma2)
}
dat <- list(Y=oven$count, 
    X=model.matrix(~ pforest + xlong, oven),
    np=3, n=nrow(oven), m=length(unique(oven$route)),
    gr=dciid(as.integer(as.factor(oven$route))), 
    pr=coef(mod),
    theta1=coefBAMspecies("OVEN", 5, 1)$sra,
    Z1=model.matrix(~JDAY + TSSR, oven),
    Sigma1=solve(vcovBAMspecies("OVEN", 5, 1)$sra),
    theta2=coefBAMspecies("OVEN", 5, 1)$edr,
    Z2=model.matrix(~pforest, oven),
    Sigma2=solve(vcovBAMspecies("OVEN", 5, 1)$edr))
dcf <- makeDcFit(model=model,
    data=dat,
    params=c("beta","log.sigma"),
    multiply=c("n","m"),
    unchanged=c("np","pr","theta1","theta2","Sigma1","Sigma2"))
cl <- makePSOCKcluster(3) # parallel computing for speed up
K <- c(1,2) # sequence for the number of clones to use
parLoadModule(cl, "glm") # load glm module for JAGS on workers
dcm <- dcmle(dcf, n.clones=K, n.iter=1000, cl=cl, partype="parchains")
stopCluster(cl) # close cluster
dcm
@
The model reveals results similar to the GLM example above, 
Proportion of forest cover around points had a significant 
positive effect on density, while longitude had a negative 
non-significant effect. The estimate of the route level random 
effect was 0.56 (SE 0.1).

\section{Prediction}

Point predictions (unconditional on the observations) and associated prediction intervals can be calculated by the same MCMC technique:
<<cache=TRUE,fig.keep='none'>>=
pmodel <- function() {
    for (i in 1:n) {
        Y[i] ~ dpois(lam[i])
        log(lam[i]) <- inprod(X[i,], tmp[1:np]) + E[i]
        E[i] ~ dnorm(0, 1/exp(tmp[np+1])^2)
    }
    tmp[1:(np+1)] ~ dmnorm(cf, Sig)
}
pf <- 10:0/10
ND <- expand.grid(pforest=pf,xlong=c(-1.8,0,1.8))
Xnew <- model.matrix(~ pforest + xlong, ND)
## grouping is random (so this is at region and not route level)
dat1 <- list(X=Xnew,
    np=3, n=nrow(Xnew), 
    cf=coef(dcm),
    Sig=solve(vcov(dcm)))
dcf2p1 <- makeDcFit(model=pmodel,
    data=dat1,
    params=c("lam"))
pm1 <- dcmle(dcf2p1, n.clones=1)
pm1 <- as.matrix(pm1)
ND$mean <- colMeans(pm1)
ND$cl1 <- apply(pm1, 2, quantile, probs=0.025)
ND$cl2 <- apply(pm1, 2, quantile, probs=0.975)
## drawing the figure
op <- par(las=1)
plot(mean ~ pforest, ND, col=as.integer(as.factor(ND$xlong)), 
    ylim=c(0, max(ND$cl2)), type="n",
    xlab="Proportion of forest", ylab="Density (males / ha)", axes=FALSE)
polygon(c(pf, rev(pf)), c(ND$cl1[ND$xlong>0], rev(ND$cl2[ND$xlong<0])),
    border=NA, col="grey")
polygon(c(pf, rev(pf)), c(ND$mean[ND$xlong>0], rev(ND$mean[ND$xlong<0])),
    border=NA, col="black")
lines(pf, ND$mean[ND$xlong==0], col="white", lwd=2)
box(bty="l")
axis(1, tck=0.02)
axis(2, tck=0.02)
par(op)
@
\begin{figure}
<<plot1,fig.show='asis',echo=FALSE>>=
op <- par(las=1)
plot(mean ~ pforest, ND, col=as.integer(as.factor(ND$xlong)), 
    ylim=c(0, max(ND$cl2)), type="n",
    xlab="Proportion of forest", ylab="Density (males / ha)", axes=FALSE)
polygon(c(pf, rev(pf)), c(ND$cl1[ND$xlong>0], rev(ND$cl2[ND$xlong<0])),
    border=NA, col="grey")
polygon(c(pf, rev(pf)), c(ND$mean[ND$xlong>0], rev(ND$mean[ND$xlong<0])),
    border=NA, col="black")
lines(pf, ND$mean[ND$xlong==0], col="white", lwd=2)
box(bty="l")
axis(1, tck=0.02)
axis(2, tck=0.02)
par(op)
@
\caption{Predicted density of Ovenbird as a function of forest cover, 
based on a generalized linear mixed model (GLMM) fitted to a 
data set from Saskatchewan, Canada. Grey shade represents 95\% 
prediction intervals incorporating uncertainties related to the 
random effect and the detectability corrections. 
Black shade indicates range of variation in mean predictions 
due to longitude, white line represents the prediction at 
the centroid of the study area.\label{fig:plot1}}
\end{figure}
Predicted density in areas with 100\% forest cover was 
1.05 males / ha ranging from 0.9 to 1.25 depending on longitude. 
This prediction almost equals the density estimate of 0.99 
(95\% confidence limits, 0.85-1.12) by Bayne (2000) 
based on territory mapping in the same region (Fig.~\ref{fig:plot1}). 

Point predictions conditional on the observations and associated prediction intervals given the observations can be calculated in a similar manner (Fig.~\ref{fig:plot2}).

<<cache=TRUE,fig.keep='none'>>=
dat2 <- list(Y=dat$Y, X=dat$X,
    np=3, n=nrow(dat$X),
    cf=coef(dcm),
    Sig=solve(vcov(dcm)))
dcf2p2 <- makeDcFit(model=pmodel,
    data=dat2,
    params=c("lam"))
pm2 <- dcmle(dcf2p2, n.clones=1)
pm2 <- as.matrix(pm2)
pp <- data.frame(mean=colMeans(pm2),
    cl1=apply(pm1, 2, quantile, probs=0.025),
    cl2=apply(pm1, 2, quantile, probs=0.975))
summary(pp)
boxplot(pp$mean~dat$Y,
    xlab="Observed counts", ylab="Predicted mean density")
@
\begin{figure}
<<plot2,fig.show='asis',echo=FALSE>>=
boxplot(pp$mean~dat$Y,
    xlab="Observed counts", ylab="Predicted mean density")
@
\caption{The relationship between observed counts and
predicted mean density conditional on the observations
based on the Ovenbird data set (males / ha).\label{fig:plot2}}
\end{figure}

\chapter{Further examples}

Further examples can be found at the BAM website 
(\href{http://www.borealbirds.ca}{http://www.borealbirds.ca}) 
under the 
\href{http://www.borealbirds.ca/avian_db/accounts.php}{Results tab}. 

These include spatial maps of expected mean abundance for 
Bird Conservation Regions (BCRs) within provinces and territories of Canada, and 
relative densities within various land cover classes per spatial units. 
Information is available for 70 bird species.

For example, the Ovenbird results 
(\href{http://www.borealbirds.ca/avian_db/accounts.php/Seiurus+aurocapilla/estimates}{%
density estimates}, 
\href{http://www.borealbirds.ca/avian_db/accounts.php/Seiurus+aurocapilla/habitat}{%
habitat associations}, and more) are available at the
\href{http://www.borealbirds.ca/avian_db/accounts.php/Seiurus+aurocapilla}{BAM website}.

\bibliographystyle{abbrvnat}
\bibliography{qpad}

\end{document}
