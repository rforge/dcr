\documentclass[article,shortnames,nojss]{jss}
\usepackage{thumbpdf}

%% need no \usepackage{Sweave.sty}
\SweaveOpts{engine = R, strip.white = TRUE, keep.source = TRUE, eps = FALSE}

\newcommand{\class}[1]{`\code{#1}'}

%\VignetteIndexEntry{dcglm tutorial}
%\VignettePackage{dcglm}
%\VignetteDepends{dclone}
%\VignetteKeywords{Bayesian statistics, data cloning, maximum likelihood inference, generalized linear models, R}

\author{P\'eter S\'olymos\\University of Alberta}
\Plainauthor{P\'eter S\'olymos}

\title{Tutorial for the \pkg{dcglm} package}
\Plaintitle{Tutorial for the dcglm package}
\Shorttitle{Tutorial for the dcglm package}

\Abstract{
  This tutorial package to demonstrate the 
  capabilities of data cloning algorithm via 
  the infrastructure provided by the \pkg{dclone} package.
  Functions reproduce main features of the \code{glm}
  base function in \proglang{R} by using data cloning.
}

\Keywords{Bayesian statistics, data cloning, maximum likelihood inference, generalized linear models, \proglang{R}}
\Plainkeywords{Bayesian statistics, data cloning, maximum likelihood inference, generalized linear models, R}

\Address{
  P\'eter S\'olymos\\
  Alberta Biodiversity Monitoring Institute\\
  Department of Biological Sciences\\
  CW 405, Biological Sciences Bldg\\
  University of Alberta\\
  Edmonton, Alberta, T6G 2E9, Canada\\
  E-mail: \email{solymos@ualberta.ca}\\
}



\begin{document}


<<echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+   ", useFancyQuotes = FALSE, width = 76)
@


\section{Introduction}

Data cloning is a statistical computing method introduced by \citet{Lele2007}.
It exploits the computational simplicity of the Markov chain Monte Carlo (MCMC) algorithms
used in the Bayesian statistical framework, but it provides valid frequentist inferences such as the maximum 
likelihood estimates and their standard errors for complex hierarchical models.
The use of the data cloning algorithm is especially straightforward for complex models, 
where the number of unknowns increases with sample size (i.e.~mixed models),
because inference and prediction procedures are often hard to implement in such situations.

The \textbf{dclone} R package \citep{Solymos2009} aims to provide low level functionality to easily implement 
more specific higher level procedures based on data cloning for users familiar with the Bayesian methodology.
This tutorial, we develop write high level functions to duplicate the some features of 
the \code{glm} base function of \proglang{R} by using
the data cloning algorithm building on the infrastructure of the \pkg{dclone} package.

\section{Data generation}

We generate random data for Poisson and Binomal GLMs. First we define the number of locations (\code{n})
and the independent covariate (\code{x}). \code{X} represents the design matrix:
<<>>=
set.seed(1234)
n <- 20
x <- runif(n, -1, 1)
X <- model.matrix(~x)
@
Parameters (\code{beta1}), linear predictor (\code{mu1}) 
and random response (\code{Y1}) for the Poisson case (log link function):
<<>>=
beta1 <- c(2, -1)
mu1 <- X %*% beta1
Y1 <- rpois(n, exp(mu1))
@
Parameters (\code{beta2}), linear predictor (\code{mu2}) 
and random response (\code{Y2}) for the Binomial (Bernoulli) case (logistic link function):
<<>>=
beta2 <- c(0, -1)
mu2 <- X %*% beta2
Y2 <- rbinom(n, 1, exp(mu2) / (1 + exp(mu2)))
@

\section{GLM based on the \code{glm} base function}

Now we fit the Poisson and Binomail GLM by using the \code{glm} base function and inspect their summaries:
<<>>=
m1 <- glm(Y1 ~ x, family=poisson)
summary(m1)
m2 <- glm(Y2 ~ x, family=binomial)
summary(m2)
@

\section{The full Bayesian model for GLM and data cloning}

Now we have to load the \pkg{dclone} package (it load its dependencies, that we also need):
<<>>=
library(dclone)
@
Here is the \proglang{JAGS} model for the Poisson case, with flat Normal priors for the regression coefficients:
<<>>=
glm.pois <- function() {
    for (i in 1:n) {
        Y[i] ~ dpois(lambda[i])
        log(lambda[i]) <- inprod(X[i,], beta[1,])
    }
    for (j in 1:np) {
        beta[1,j] ~ dnorm(0, 0.001)
    }
}
@
The data will be represented as a list (if we want to use data cloning consistently, 
we have to use the list format instead of the global environment):
<<>>=
dat1 <- list(n = length(Y1), Y = Y1, X = X, np = ncol(X))
str(dat1)
@
Now let's clone the data set (note that \code{n} should be multiplies, while
\code{np} must remain unchanged):
<<>>=
n.clones <- 5
dcdat1 <- dclone(dat1, n.clones, multiply = "n", unchanged = "np")
str(dcdat1)
@
To fit the model to the data with data cloning is as easy as this:
<<>>=
mod1 <- jags.fit(dcdat1, "beta", glm.pois)
summary(mod1)
@
Le's compare this \code{mcmc.list} (more accurately an \code{mcmc.list.dc}) object with the \code{glm} results:
<<>>=
cbind(glm.estimates=coef(m1), glm.se=summary(m1)$coefficients[,2],
    dc.estimates=coef(mod1), dc.se=dcsd(mod1))
@

Here is the \proglang{JAGS} model for the Binomial (Bernoulli, because \code{k}=1) case:
<<>>=
glm.bin <- function() {
    for (i in 1:n) {
        Y[i] ~ dbin(p[i], k)
        logit(p[i]) <- inprod(X[i,], beta[1,])
    }
    for (j in 1:np) {
        beta[1,j] ~ dnorm(0, 0.001)
    }
}
@
Putting together the data set is similar to the Poisson case:
<<>>=
dat2 <- list(n = length(Y2), Y = Y2, k = 1, X = X, np = ncol(X))
str(dat2)
@
but data cloning setup is a bit different. We don't have to repeat the data vectors
or columns in the design matrix \code{n.clones} times, because there is an easier way.
We multiply \code{Y} and \code{k} with \code{n.clones} and leave the other elements unchanged:
<<>>=
dcdat2 <- dclone(dat, n.clones, multiply = c("Y","k"), unchanged = c("n", "np", "X"))
str(dcdat2)
@
Now fit the model to the data with data cloning:
<<>>=
mod2 <- jags.fit(dcdat2, "beta", glm.bin)
summary(mod2)
@
and compare results with \code{glm} results:
<<>>=
cbind(glm.estimates=coef(m2), glm.se=summary(m2)$coefficients[,2],
    dc.estimates=coef(mod2), dc.se=dcsd(mod2))
@

\section{The \code{custommodel} function}

The \code{custommodel} function enables us to resuse the same \proglang{JAGS} model
with minor modifications. For example we combine the above Poisson and Binomial model into one.
<<>>=
glm.model <- function() {
    for (i in 1:n) {
        Y[i] ~ dpois(lambda[i])
        Y[i] ~ dbin(p[i], k)
        log(lambda[i]) <- inprod(X[i,], beta[1,])
        logit(p[i]) <- inprod(X[i,], beta[1,])
    }
    for (j in 1:np) {
        beta[1,j] ~ dnorm(0, 0.001)
    }
}
@
If we want to use this in the \code{jags.fit} function, it would give the error message
about the attempt to define the nodes more than onece. To avoid this, we tell the function
which lines ahould be excluded:
<<>>=
custommodel(glm.model, c(4,6))
custommodel(glm.model, c(3,5))
@
so eventually we get back our original models. But these are not functions, but character vectors
of the class \code{custommodel}. \code{jags.fit} will recognize this.

Why do we want to complicate our lives with the \code{custommodel}?
Because \code{dpois}, \code{dbin}, and \code{inprod} are not recognised
as valid \R objects or functions. So if our aim is to make an \R package
that passes the rather strict \code{R CMD check}, so won't be published
at the Comprehensive \R Archive Network (CRAN). A way to overcome this
situation is to define fake objects as e.g.~\code{inprod <- function() NULL},
but this option should be regaded as ugly and inefficient (unnecessary) as compared to 
a clean \code{custommodel} approach that will be presented in the next section.

\section{The main function: \code{dcglm}}

Here our main function for the data cloning based estimating procedure
for the Poisson and Binomial GLMs:
<<>>=
dcglm <- function(formula, data = parent.frame(), family=c("poisson", "binomial"), n.clones=5, ...){
    glm.model <- c("model {",
                   "    for (i in 1:n) {",
                   "        Y[i] ~ dpois(lambda[i])",
                   "        Y[i] ~ dbin(p[i], k)",
                   "        log(lambda[i]) <- inprod(X[i,], beta[1,])",
                   "        logit(p[i]) <- inprod(X[i,], beta[1,])",
                   "    }",
                   "    for (j in 1:np) {",
                   "        beta[1,j] ~ dnorm(0, 0.001)",
                   "    }",
                   "}")
    family <- match.arg(family)
    lhs <- formula[[2]]
    Y <- eval(lhs, data)
    formula[[2]] <- NULL
    rhs <- model.frame(formula, data)
    X <- model.matrix(attr(rhs, "terms"), rhs)
    dat <- list(n = length(Y), Y = Y, X = X, np = ncol(X), k = 1)
    if (family == "poisson") {
        model <- model <- custommodel(glm.model, c(4,6))
        dcdat <- dclone(dat, n.clones, multiply = "n", unchanged = "np")
    } else {
        model <- custommodel(glm.model, c(3,5))
        dcdat <- dclone(dat, n.clones, multiply = c("Y","k"), unchanged = c("n", "np", "X"))
    }
    mod <- jags.fit(dcdat, "beta", model, ...)
    COEF <- coef(mod)
    SE <- dcsd(mod)
    names(COEF) <- names(SE) <- colnames(X)
    mu <- X %*% COEF
    if (family == "poisson") {
        fitval <- exp(mu)
        ll <- sum(log(fitval^Y * exp(-fitval)) - log(factorial(Y)))
    } else {
        fitval <- exp(mu) / (1 + exp(mu))
        ll <- sum(log(choose(1, Y) * fitval^Y * (1-fitval)^(1-Y)))
    }
    rval <- list(call=match.call(),
        mcmc = mod,
        y = Y,
        x = rhs, 
        model = X,
        fitted.values = fitval,
        linear.predictors = mu,
        formula = formula,
        coefficients = COEF,
        std.error = SE,
        loglik = ll,
        family = family,
        df.residual = length(Y) - length(COEF),
        df.null = length(Y) - 1)
    class(rval) <- c("dcglm")
    rval
}
@

Let'g go through it step-by-step as pseudo-code:

\begin{enumerate}
  \item \code{glm.model} is the \code{custommodel} version of the \proglang{BUGS} model,
    unifying the Poisson and Binomial cases, as we have seen before.
  \item The \code{family} argument is recognized, and as a result, it can be given not
    only in full (e.g.~\code{family = "p"} is equivalent of \code{family = "poisson"}).
  \item \code{lhs} is the left-hand-side of the formula, \code{Y} is the value as a result
    of evaluating \code{lhs} in \code{data} (that is the parent frame, which is usually the global
    environment if not called from inside of a function).
  \item \code{formula[[2]] <- NULL} removes the left-hand-side from the formula.
  \item \code{rhs} is the right-hand-side, that is a model frame with variables
    defined in \code{data}.
  \item The design matrix \code{X} is a result of using the \code{"terms"} attribute
    of \code{rhs} and evaluated in \code{rhs}.
  \item \code{dat} is the Bayesian data representation.
  \item The \code{model} and the data cloned data representation
    (\code{dcdat}) depends on the \code{family} argument.
  \item \code{mod} is the fitted \code{mcmc.list} object. Dots (\code{...}) represents
    all the additional arguments that can be passed, including \code{n.update}, \code{n.iter},
    and \code{n.chains}.
  \item \code{COEF} is the \code{coef} method evaluated on the \code{mcmc.list} object \code{mod}.
    \code{SD} is the data cloned standard error (scaled by $\sqrt{k}$). 
    Names of \code{COEF} and \code{SD} follow column names of \code{X}.
  \item \code{mu} is the linear predictor (on log/logit scale), while \code{fitval} is the
    fitted value (response scale after using the appropriate inverse link function) and
    \code{ll} is the log-likelihood calculated from the probability mass function.
  \item \code{rval} is the return value, that is a list with elements commonly applied in
    objects representing model fit (cf.~for example element names with \code{names(m1)}):
    \begin{description}
        \item[call] the function call,
        \item[mcmc] the fitted \code{mcmc.list} object,
        \item[y] the response,
        \item[x] the model frame (right-hand-side), 
        \item[model] the design matrix,
        \item[fitted.values] fitted values,
        \item[linear.predictors] linear predictors,
        \item[formula] the formula argument of the call,
        \item[coefficients] means of the joint posterior distribution (maximum likelihood estimates),
        \item[std.error] standard errors of the MLE,
        \item[loglik] log-likelihood,
        \item[family] family argument of the call,
        \item[df.residual] residual degrees of freedom,
        \item[df.null] degrees of freedom in the null model.
    \end{description}
  \item Finally, we attach the class attribute and return \code{rval}.
\end{enumerate}

Fun, isn't it? See if it is actually working:
<<>>=
dcm1 <- dcglm(Y1 ~ x)
dcm2 <- dcglm(Y2 ~ x, family = "binomial")
@

If we are about to inspect these objects, well, it is a mess without some
additional helper functions. The most basic such functions (called methods in \R
jargon) are \code{print} and \code{summary}. For our convenience, 
we also define some other methods, too. These are based on the so called S3
method dispatch system. That is, if a generic function is defined,
we can add class specific methods to it.

In our case, the most simple methods are the \code{coef} and \code{fitted},
because these only extract an element from the objects:
<<>>=
coef.dcglm <- function(object, ...) object$coefficients
fitted.dcglm <- function(object, ...) object$fitted.values
@
Compare with the \code{glm} results:
<<>>=
rbind(glm=coef(m1), dcglm=coef(dcm1))
rbind(glm=coef(m2), dcglm=coef(dcm2))
rbind(glm=fitted(m1), dcglm=fitted(dcm1))
rbind(glm=fitted(m2), dcglm=fitted(dcm2))
@

For the \code{logLik} method, it is necessary to follow the standard
rules, because AIC calculations depend on this method (this means, that we don't have to
define a method for AIC if the \code{logLik} method exists for a class):
<<>>=
logLik.dcglm <- function (object, ...)
    structure(object$loglik,
        df = object$df.null + 1 - object$df.residual,
        nobs = object$df.null + 1,
        class = "logLik")
@
Compare with the \code{glm} results:
<<>>=
logLik(m1)
logLik(dcm1)
logLik(m2)
logLik(dcm2)
AIC(m1, dcm1, m2, dcm2)
@

Now it is possible to write the \code{print} method:
<<>>=
print.dcglm <- function(x, digits = max(3, getOption("digits") - 3), ...) {
    cat("\nCall: ", deparse(x$call), "\n\n")
    cat("Coefficients:\n")
    print.default(format(x$coefficients, digits = digits), print.gap = 2, quote = FALSE)
    cat("\nDegrees of Freedom:", x$df.null, "Total (i.e. Null); ", x$df.residual, "Residual\n")
    cat("Log Likelihood:\t   ", format(signif(x$loglik, digits)), "\n")
    invisible(x)
}
@

Let's have a look at the resulting objects of our \code{dcglm} function:
<<>>=
dcm1
dcm2
@
Well done so far!

\section{Methods for inference}

For the \code{confint}, we use the asymptotic normality result
of the data cloning theory \citep{Lele2007}, and the \code{confint} method defined for the
data cloned \code{mcmc.list} part of the fitted model object:
<<>>=
confint.dcglm <- function(object, parm, level = 0.95, ...) {
    rval <- confint(object$mcmc, parm, level, ...)
    rownames(rval) <- names(coef(object))
    rval
}
@

The \code{summary} methods returns the ML estimates, data cloning
standard errors, and Wald-type $z$ statistics and $p$-values:
<<>>=
summary.dcglm <- function(object, ...){
    COEF <- coef(object)
    SE <- object$std.error
    z <- COEF / SE
    p <-  2 * pnorm(-abs(z))
    stab <- cbind("Estimate" = COEF, "Std. Error" = SE,
        "z value" = z, "Pr(>|z|)" = p)
    rval <- list(call = object$call, 
        coefficients = stab, 
        loglik = object$loglik,
        df.residual = object$df.residual,
        df.null = object$df.null)
    class(rval) <- "summary.dcglm"
    rval
}
@
The return value here is also a list, repeating some of the elements of
the fitted \code{object}. To appropriately format the summary,
we use the \code{print} method for the object class \code{summary.dcglm}:
<<>>=
print.summary.dcglm <- 
function (x, digits = max(3, getOption("digits") - 3), 
    signif.stars = getOption("show.signif.stars"), ...) 
{
    cat("\nCall:\n")
    cat(paste(deparse(x$call), sep = "\n", collapse = "\n"), "\n", sep = "")
    cat("\nCoefficients:\n")
    printCoefmat(x$coefficients, digits = digits, signif.stars = signif.stars, na.print = "NA", ...)
    cat("\nDegrees of Freedom:", x$df.null, "Total (i.e. Null); ", x$df.residual, "Residual\n")
    cat("Log Likelihood:\t   ", format(signif(x$loglik, digits)), "\n")
    invisible(x)
}
@
Summaries of the \code{glm()} results and our models:
<<>>=
summary(m1)
summary(dcm1)
summary(m2)
summary(dcm2)
@
Piece of cake!

\section{Prediction based on the joint posterior distribution}

In the prediction, we use MCMC. The likelihood part of the 
\proglang{BUGS} model for the prediction is the same as for the
estimation. The only difference is in the prior specification:
<<>>=
glm.pred <- function() {
    for (i in 1:n) {
        Y[i] ~ dpois(z[i])
        Y[i] ~ dbin(z[i], k)
        log(z[i]) <- mu[i]
        logit(z[i]) <- mu[i]
        mu[i] <- inprod(X[i,], beta[1,])
    }
    beta[1,1:np] <- mvn[1:np]
    mvn[1:np] ~ dmnorm(coefs[], prec[,])
}
@

Poisson and Binomial case:
<<>>=
custommodel(glm.pred, c(4,6))
custommodel(glm.pred, c(3,5))
@

jags.fit version here from predict

\section{Methods for prediction}

It is similar with the \code{vcov} method, we simply use the
\code{vcov} method defined for the \code{mcmc.list} part of 
the fitted model object and do some cosmetics on the names:
<<>>=
vcov.dcglm <- function(object, ...) {
    rval <- vcov(object$mcmc, ...)
    rownames(rval) <- colnames(rval) <- names(coef(object))
    rval
}
@

The \code{predict} function:
<<>>=
predict.dcglm <- function(object, newdata = NULL, type = c("link", "response"), se = FALSE, ...){
    glm.pred <- c("model {",
           "    for (i in 1:n) {",
           "        Y[i] ~ dpois(z[i])",
           "        Y[i] ~ dbin(z[i], k)",
           "        log(z[i]) <- mu[i]",
           "        logit(z[i]) <- mu[i]",
           "        mu[i] <- inprod(X[i,], beta[1,])",
           "    }",
           "    beta[1,1:np] <- mvn[1:np]",
           "    mvn[1:np] ~ dmnorm(coefs[], prec[,])",
           "    }")
    prec <- make.symmetric(solve(vcov(object)))
    coefs <- coef(object)
    if (is.null(newdata)) {
        X <- object$model
    } else {
        rhs <- model.frame(object$formula, newdata)
        X <- model.matrix(attr(rhs, "terms"), rhs)
    }
    type <- match.arg(type)
    params <- switch(type,
        "link" = "mu",
        "response" = "z")
    model <- switch(object$family,
        "poisson" = custommodel(glm.pred, c(4,6)),
        "binomial" = custommodel(glm.pred, c(3,5)))
    prdat <- list(n = nrow(X), X = X, 
        np = ncol(X), k = 1, coefs = coefs, prec = prec)
    prval <- jags.fit(prdat, params, model, ...)
    if (!se) {
        rval <- coef(prval)
    } else {
        rval <- list(fit = coef(prval), 
            se.fit = mcmcapply(prval, sd))
    }
    rval
}
@

Pseudo-code for \code{predict}:

Then do the prediction and graphically compare

\section{Making the \code{dcglm} package}

The easiest part now comes:
<<eval=false>>=
package.skeleton("dcglm", c("coef.dcglm","confint.dcglm","dcglm",
    "fitted.dcglm","logLik.dcglm","predict.dcglm",
    "print.dcglm","print.summary.dcglm","summary.dcglm","vcov.dcglm"))
@

Follow this workflow for your own model and estimating procedure, then edit the files (read the \emph{Writing \R Extensions} manual) in the package directory, run \code{R CMD check}, and ditribute your package.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{dcglm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

Todo:
- edit Rd files of the package
